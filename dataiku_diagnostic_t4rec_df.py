#!/usr/bin/env python3
"""
DIAGNOSTIC COMPLET: TABLE T4REC_DF (384 COLONNES)
===============================================

Script de diagnostic pour analyser la table t4_rec_df dans Dataiku
et vÃ©rifier la compatibilitÃ© avec le systÃ¨me T4Rec.
"""

import dataiku
import numpy as np
import pandas as pd
import torch
from collections import Counter
import traceback


def load_and_analyze_t4rec_df():
    """Charge et analyse complÃ¨tement la table t4_rec_df."""

    print("ğŸ” DIAGNOSTIC COMPLET: TABLE T4REC_DF")
    print("=" * 60)

    try:
        # 1. CHARGEMENT DE LA TABLE
        print("\nğŸ“‚ 1. CHARGEMENT DE LA TABLE")
        print("-" * 30)

        dataset = dataiku.Dataset("t4_rec_df")
        df = dataset.get_dataframe()

        print(f"âœ… Table chargÃ©e avec succÃ¨s:")
        print(f"  â€¢ Lignes: {len(df):,}")
        print(f"  â€¢ Colonnes: {len(df.columns):,}")
        print(
            f"  â€¢ Taille mÃ©moire: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB"
        )

        # 2. ANALYSE DES TYPES DE COLONNES
        print("\nğŸ”¢ 2. ANALYSE DES TYPES DE COLONNES")
        print("-" * 30)

        type_analysis = analyze_column_types(df)

        # 3. ANALYSE DES DIMENSIONS (ARRAYS)
        print("\nğŸ“ 3. ANALYSE DES DIMENSIONS DES ARRAYS")
        print("-" * 30)

        dimension_analysis = analyze_array_dimensions(df)

        # 4. VÃ‰RIFICATIONS SPÃ‰CIFIQUES T4REC
        print("\nğŸ¯ 4. VÃ‰RIFICATIONS SPÃ‰CIFIQUES T4REC")
        print("-" * 30)

        t4rec_analysis = analyze_t4rec_compatibility(df)

        # 5. VÃ‰RIFICATIONS DLPACK/MERLIN
        print("\nğŸ”„ 5. VÃ‰RIFICATIONS DLPACK/MERLIN")
        print("-" * 30)

        dlpack_analysis = analyze_dlpack_merlin_compatibility(df)

        # 6. RAPPORT FINAL
        print("\nğŸ“Š 6. RAPPORT FINAL")
        print("-" * 30)

        final_report = generate_final_report(
            df, type_analysis, dimension_analysis, t4rec_analysis, dlpack_analysis
        )

        return {
            "dataframe": df,
            "type_analysis": type_analysis,
            "dimension_analysis": dimension_analysis,
            "t4rec_analysis": t4rec_analysis,
            "dlpack_analysis": dlpack_analysis,
            "final_report": final_report,
        }

    except Exception as e:
        print(f"âŒ ERREUR lors du chargement:")
        print(f"  â€¢ Erreur: {str(e)}")
        print(f"  â€¢ DÃ©tails: {traceback.format_exc()}")
        return None


def analyze_column_types(df):
    """Analyse les types de toutes les colonnes."""

    type_stats = {
        "array_columns": [],
        "numeric_columns": [],
        "string_columns": [],
        "object_columns": [],
        "bool_columns": [],
        "datetime_columns": [],
        "problematic_columns": [],
    }

    print(f"ğŸ“Š Analyse de {len(df.columns)} colonnes...")

    for i, col in enumerate(df.columns):
        try:
            dtype = df[col].dtype
            sample_value = df[col].iloc[0] if len(df) > 0 else None

            # Classification des types
            if dtype == "object":
                # VÃ©rifier si c'est des arrays
                if sample_value is not None and isinstance(
                    sample_value, (list, np.ndarray)
                ):
                    type_stats["array_columns"].append(
                        {
                            "column": col,
                            "dtype": str(dtype),
                            "sample_type": type(sample_value).__name__,
                            "sample_value": str(sample_value)[:100] + "..."
                            if len(str(sample_value)) > 100
                            else str(sample_value),
                        }
                    )
                else:
                    type_stats["object_columns"].append(
                        {
                            "column": col,
                            "dtype": str(dtype),
                            "sample_value": str(sample_value)[:50] + "..."
                            if sample_value and len(str(sample_value)) > 50
                            else str(sample_value),
                        }
                    )

            elif "int" in str(dtype):
                type_stats["numeric_columns"].append(
                    {
                        "column": col,
                        "dtype": str(dtype),
                        "min": df[col].min(),
                        "max": df[col].max(),
                    }
                )

            elif "float" in str(dtype):
                type_stats["numeric_columns"].append(
                    {
                        "column": col,
                        "dtype": str(dtype),
                        "min": df[col].min(),
                        "max": df[col].max(),
                    }
                )

            elif "bool" in str(dtype):
                type_stats["bool_columns"].append(
                    {"column": col, "dtype": str(dtype), "true_count": df[col].sum()}
                )

            elif "datetime" in str(dtype):
                type_stats["datetime_columns"].append(
                    {
                        "column": col,
                        "dtype": str(dtype),
                        "min_date": df[col].min(),
                        "max_date": df[col].max(),
                    }
                )

            else:
                type_stats["problematic_columns"].append(
                    {
                        "column": col,
                        "dtype": str(dtype),
                        "sample_value": str(sample_value),
                    }
                )

            # Progress
            if (i + 1) % 50 == 0:
                print(f"  â€¢ AnalysÃ© {i + 1}/{len(df.columns)} colonnes...")

        except Exception as e:
            type_stats["problematic_columns"].append(
                {"column": col, "dtype": "ERROR", "error": str(e)}
            )

    # RÃ©sumÃ©
    print(f"\nğŸ“‹ RÃ‰SUMÃ‰ DES TYPES:")
    print(f"  â€¢ Arrays/Lists: {len(type_stats['array_columns'])}")
    print(f"  â€¢ NumÃ©riques: {len(type_stats['numeric_columns'])}")
    print(f"  â€¢ Strings/Objects: {len(type_stats['object_columns'])}")
    print(f"  â€¢ BoolÃ©ens: {len(type_stats['bool_columns'])}")
    print(f"  â€¢ Dates: {len(type_stats['datetime_columns'])}")
    print(f"  â€¢ ProblÃ©matiques: {len(type_stats['problematic_columns'])}")

    # DÃ©tails des colonnes arrays
    if type_stats["array_columns"]:
        print(f"\nğŸ” DÃ‰TAILS COLONNES ARRAYS (premiÃ¨res 5):")
        for i, arr_col in enumerate(type_stats["array_columns"][:5]):
            print(f"  {i + 1}. {arr_col['column']}")
            print(f"     â€¢ Type: {arr_col['sample_type']}")
            print(f"     â€¢ Exemple: {arr_col['sample_value']}")

    # Colonnes problÃ©matiques
    if type_stats["problematic_columns"]:
        print(f"\nâš ï¸ COLONNES PROBLÃ‰MATIQUES:")
        for prob_col in type_stats["problematic_columns"][:5]:
            print(
                f"  â€¢ {prob_col['column']}: {prob_col.get('error', prob_col['dtype'])}"
            )

    return type_stats


def analyze_array_dimensions(df):
    """Analyse les dimensions des arrays dans les colonnes."""

    dimension_stats = {
        "array_dimensions": {},
        "uniform_lengths": {},
        "dimension_issues": [],
    }

    # Identifier les colonnes avec des arrays
    array_columns = []
    for col in df.columns:
        try:
            sample = df[col].iloc[0]
            if isinstance(sample, (list, np.ndarray)):
                array_columns.append(col)
        except:
            continue

    print(f"ğŸ“Š Analyse des dimensions pour {len(array_columns)} colonnes arrays...")

    for i, col in enumerate(array_columns[:20]):  # Limite Ã  20 pour la dÃ©mo
        try:
            # Analyser quelques Ã©chantillons
            sample_sizes = []
            sample_shapes = []

            for idx in range(min(100, len(df))):  # Ã‰chantillon de 100 lignes
                try:
                    value = df[col].iloc[idx]
                    if isinstance(value, (list, np.ndarray)):
                        if isinstance(value, list):
                            sample_sizes.append(len(value))
                            sample_shapes.append(f"1D-{len(value)}")
                        else:
                            sample_sizes.append(value.size)
                            sample_shapes.append(str(value.shape))
                except:
                    continue

            if sample_sizes:
                dimension_stats["array_dimensions"][col] = {
                    "min_size": min(sample_sizes),
                    "max_size": max(sample_sizes),
                    "avg_size": np.mean(sample_sizes),
                    "unique_shapes": list(set(sample_shapes[:10])),  # Top 10 shapes
                    "is_uniform": len(set(sample_sizes)) == 1,
                }

                # VÃ©rifier l'uniformitÃ©
                if len(set(sample_sizes)) == 1:
                    dimension_stats["uniform_lengths"][col] = sample_sizes[0]
                else:
                    dimension_stats["dimension_issues"].append(
                        {
                            "column": col,
                            "issue": "Variable lengths",
                            "sizes_range": f"{min(sample_sizes)}-{max(sample_sizes)}",
                        }
                    )

            if (i + 1) % 10 == 0:
                print(
                    f"  â€¢ AnalysÃ© {i + 1}/{min(20, len(array_columns))} colonnes arrays..."
                )

        except Exception as e:
            dimension_stats["dimension_issues"].append(
                {"column": col, "issue": "Analysis error", "error": str(e)}
            )

    # RÃ©sumÃ©
    print(f"\nğŸ“‹ RÃ‰SUMÃ‰ DES DIMENSIONS:")
    print(f"  â€¢ Colonnes arrays analysÃ©es: {len(dimension_stats['array_dimensions'])}")
    print(f"  â€¢ Longueurs uniformes: {len(dimension_stats['uniform_lengths'])}")
    print(f"  â€¢ ProblÃ¨mes dÃ©tectÃ©s: {len(dimension_stats['dimension_issues'])}")

    # DÃ©tails
    if dimension_stats["uniform_lengths"]:
        print(f"\nâœ… COLONNES AVEC LONGUEURS UNIFORMES:")
        for col, length in list(dimension_stats["uniform_lengths"].items())[:5]:
            print(f"  â€¢ {col}: longueur {length}")

    if dimension_stats["dimension_issues"]:
        print(f"\nâš ï¸ PROBLÃˆMES DE DIMENSIONS:")
        for issue in dimension_stats["dimension_issues"][:5]:
            print(f"  â€¢ {issue['column']}: {issue['issue']}")

    return dimension_stats


def analyze_t4rec_compatibility(df):
    """VÃ©rifie la compatibilitÃ© avec T4Rec."""

    t4rec_checks = {
        "tensor_convertible": [],
        "sequence_candidates": [],
        "target_ready": [],
        "compatibility_issues": [],
    }

    print("ğŸ¯ VÃ©rification compatibilitÃ© T4Rec...")

    # VÃ©rifier les colonnes pour la conversion tensor
    array_columns = []
    for col in df.columns:
        try:
            sample = df[col].iloc[0]
            if isinstance(sample, (list, np.ndarray)):
                array_columns.append(col)
        except:
            continue

    print(f"ğŸ“Š Test de conversion sur {min(10, len(array_columns))} colonnes arrays...")

    for col in array_columns[:10]:  # Test sur 10 colonnes
        try:
            # Test de conversion en array numpy
            sample_data = df[col].iloc[:5].tolist()  # 5 Ã©chantillons

            # VÃ©rifier si c'est convertible en array uniforme
            try:
                test_array = np.array(sample_data)
                if test_array.ndim == 2:  # SÃ©quences 2D
                    # Test de conversion tensor
                    test_tensor = torch.LongTensor(test_array)

                    # Test de crÃ©ation input/target pairs
                    if test_array.shape[1] > 1:
                        inputs = test_tensor[:, :-1]
                        targets = test_tensor[:, 1:]

                        t4rec_checks["tensor_convertible"].append(col)
                        t4rec_checks["sequence_candidates"].append(
                            {
                                "column": col,
                                "shape": test_array.shape,
                                "seq_length": test_array.shape[1],
                                "can_create_targets": True,
                            }
                        )

                        if inputs.shape == targets.shape:
                            t4rec_checks["target_ready"].append(col)

                        print(
                            f"  âœ… {col}: Compatible T4Rec (shape: {test_array.shape})"
                        )
                    else:
                        t4rec_checks["compatibility_issues"].append(
                            {
                                "column": col,
                                "issue": "SÃ©quence trop courte pour targets",
                            }
                        )
                else:
                    t4rec_checks["compatibility_issues"].append(
                        {
                            "column": col,
                            "issue": f"Dimension incorrecte: {test_array.ndim}D",
                        }
                    )

            except Exception as tensor_error:
                t4rec_checks["compatibility_issues"].append(
                    {
                        "column": col,
                        "issue": f"Conversion tensor Ã©chouÃ©e: {str(tensor_error)}",
                    }
                )

        except Exception as e:
            t4rec_checks["compatibility_issues"].append(
                {"column": col, "issue": f"Erreur analyse: {str(e)}"}
            )

    # RÃ©sumÃ©
    print(f"\nğŸ“‹ COMPATIBILITÃ‰ T4REC:")
    print(f"  â€¢ Convertibles en tensor: {len(t4rec_checks['tensor_convertible'])}")
    print(f"  â€¢ Candidats sÃ©quences: {len(t4rec_checks['sequence_candidates'])}")
    print(f"  â€¢ PrÃªts pour targets: {len(t4rec_checks['target_ready'])}")
    print(f"  â€¢ ProblÃ¨mes: {len(t4rec_checks['compatibility_issues'])}")

    return t4rec_checks


def analyze_dlpack_merlin_compatibility(df):
    """VÃ©rifie la compatibilitÃ© DLPack/Merlin."""

    dlpack_checks = {
        "dlpack_compatible": [],
        "merlin_compatible": [],
        "conversion_needed": [],
        "incompatible": [],
    }

    print("ğŸ”„ VÃ©rification compatibilitÃ© DLPack/Merlin...")

    for col in df.columns[:20]:  # Test sur 20 colonnes
        try:
            dtype = df[col].dtype
            sample = df[col].iloc[0]

            # Check DLPack compatibility
            if dtype in [np.int32, np.int64, np.float32]:
                dlpack_checks["dlpack_compatible"].append(col)
                dlpack_checks["merlin_compatible"].append(col)
            elif "int" in str(dtype) or "float" in str(dtype):
                dlpack_checks["conversion_needed"].append(
                    {
                        "column": col,
                        "current_type": str(dtype),
                        "target_type": "int32" if "int" in str(dtype) else "float32",
                    }
                )
            elif dtype == "object":
                if isinstance(sample, (list, np.ndarray)):
                    dlpack_checks["conversion_needed"].append(
                        {
                            "column": col,
                            "current_type": "object (array)",
                            "target_type": "need array analysis",
                        }
                    )
                else:
                    dlpack_checks["incompatible"].append(
                        {"column": col, "issue": "Object type non-array"}
                    )
            else:
                dlpack_checks["incompatible"].append(
                    {"column": col, "issue": f"Type {dtype} non supportÃ©"}
                )

        except Exception as e:
            dlpack_checks["incompatible"].append(
                {"column": col, "issue": f"Erreur: {str(e)}"}
            )

    # RÃ©sumÃ©
    print(f"\nğŸ“‹ COMPATIBILITÃ‰ DLPACK/MERLIN:")
    print(f"  â€¢ Compatible directement: {len(dlpack_checks['dlpack_compatible'])}")
    print(f"  â€¢ Compatible Merlin: {len(dlpack_checks['merlin_compatible'])}")
    print(f"  â€¢ Conversion nÃ©cessaire: {len(dlpack_checks['conversion_needed'])}")
    print(f"  â€¢ Incompatible: {len(dlpack_checks['incompatible'])}")

    return dlpack_checks


def generate_final_report(
    df, type_analysis, dimension_analysis, t4rec_analysis, dlpack_analysis
):
    """GÃ©nÃ¨re le rapport final avec recommandations."""

    print("ğŸ“Š GÃ‰NÃ‰RATION DU RAPPORT FINAL")

    report = {
        "summary": {
            "total_columns": len(df.columns),
            "total_rows": len(df),
            "array_columns": len(type_analysis["array_columns"]),
            "t4rec_ready": len(t4rec_analysis["target_ready"]),
            "major_issues": 0,
        },
        "recommendations": [],
        "next_steps": [],
    }

    # Calcul des problÃ¨mes majeurs
    major_issues = 0
    major_issues += len(type_analysis["problematic_columns"])
    major_issues += len(dimension_analysis["dimension_issues"])
    major_issues += len(t4rec_analysis["compatibility_issues"])
    major_issues += len(dlpack_analysis["incompatible"])

    report["summary"]["major_issues"] = major_issues

    # Recommandations
    if len(t4rec_analysis["target_ready"]) > 0:
        report["recommendations"].append(
            f"âœ… EXCELLENT: {len(t4rec_analysis['target_ready'])} colonnes prÃªtes pour T4Rec"
        )

    if len(type_analysis["array_columns"]) > len(t4rec_analysis["target_ready"]):
        diff = len(type_analysis["array_columns"]) - len(t4rec_analysis["target_ready"])
        report["recommendations"].append(
            f"âš ï¸ {diff} colonnes arrays nÃ©cessitent des ajustements pour T4Rec"
        )

    if len(dlpack_analysis["conversion_needed"]) > 0:
        report["recommendations"].append(
            f"ğŸ”„ {len(dlpack_analysis['conversion_needed'])} colonnes nÃ©cessitent conversion DLPack"
        )

    if major_issues > 0:
        report["recommendations"].append(
            f"âŒ {major_issues} problÃ¨mes majeurs Ã  rÃ©soudre"
        )

    # Ã‰tapes suivantes
    if len(t4rec_analysis["target_ready"]) > 0:
        report["next_steps"].append("1. Tester model_trainer avec colonnes prÃªtes")
        report["next_steps"].append("2. Convertir autres colonnes arrays")
    else:
        report["next_steps"].append("1. Analyser structure des arrays")
        report["next_steps"].append("2. Standardiser dimensions")
        report["next_steps"].append("3. CrÃ©er paires input/target")

    if len(dlpack_analysis["conversion_needed"]) > 0:
        report["next_steps"].append("4. Convertir types pour DLPack")

    # Affichage du rapport
    print(f"\n" + "=" * 50)
    print(f"ğŸ“Š RAPPORT FINAL")
    print(f"=" * 50)

    print(f"\nğŸ“ˆ RÃ‰SUMÃ‰:")
    for key, value in report["summary"].items():
        print(f"  â€¢ {key}: {value:,}")

    print(f"\nğŸ’¡ RECOMMANDATIONS:")
    for i, rec in enumerate(report["recommendations"], 1):
        print(f"  {i}. {rec}")

    print(f"\nğŸš€ PROCHAINES Ã‰TAPES:")
    for step in report["next_steps"]:
        print(f"  {step}")

    # Score de compatibilitÃ©
    if report["summary"]["array_columns"] > 0:
        compatibility_score = (
            report["summary"]["t4rec_ready"] / report["summary"]["array_columns"]
        ) * 100
        print(f"\nğŸ¯ SCORE DE COMPATIBILITÃ‰ T4REC: {compatibility_score:.1f}%")

        if compatibility_score >= 80:
            print("  âœ… EXCELLENT - PrÃªt pour production")
        elif compatibility_score >= 60:
            print("  âš ï¸ BON - Quelques ajustements nÃ©cessaires")
        elif compatibility_score >= 40:
            print("  ğŸ”„ MOYEN - Travail de prÃ©paration requis")
        else:
            print("  âŒ FAIBLE - Restructuration majeure nÃ©cessaire")

    return report


# SCRIPT PRINCIPAL POUR DATAIKU
if __name__ == "__main__":
    print("ğŸš€ LANCEMENT DU DIAGNOSTIC T4REC_DF")
    print("=" * 60)

    # ExÃ©cuter l'analyse complÃ¨te
    results = load_and_analyze_t4rec_df()

    if results:
        print(f"\nğŸ‰ DIAGNOSTIC TERMINÃ‰ AVEC SUCCÃˆS!")
        print(f"ğŸ“ RÃ©sultats disponibles dans la variable 'results'")

        # Sauvegarder le rapport (optionnel)
        # import pickle
        # with open('/tmp/t4rec_diagnostic_report.pkl', 'wb') as f:
        #     pickle.dump(results, f)
        # print(f"ğŸ’¾ Rapport sauvegardÃ©: /tmp/t4rec_diagnostic_report.pkl")

    else:
        print(f"\nâŒ DIAGNOSTIC Ã‰CHOUÃ‰")
        print(f"VÃ©rifiez que la table 't4_rec_df' existe dans Dataiku")
