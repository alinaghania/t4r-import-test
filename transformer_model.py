"""
MODULE: MOD√àLE TRANSFORMER POUR RECOMMANDATION
==============================================

Ce module d√©finit l'architecture du mod√®le Transformer pour
la recommandation de produits.

"""

import torch
import torch.nn as nn
import logging
from typing import Optional

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TransformerRecommendationModel(nn.Module):
    """
    Mod√®le Transformer pour la recommandation de produits.

    Architecture:
    1. Couche d'embedding pour les produits
    2. Transformer Encoder pour capturer les d√©pendances s√©quentielles
    3. Couche de classification pour pr√©dire le prochain produit

    Le mod√®le utilise l'approche "next-item prediction" o√π l'objectif
    est de pr√©dire le prochain produit dans une s√©quence donn√©e.
    """

    def __init__(
        self,
        num_items: int,
        embedding_dim: int = 64,
        seq_length: int = 10,
        num_heads: int = 4,
        num_layers: int = 2,
        dropout: float = 0.1,
        feedforward_dim: Optional[int] = None,
    ):
        """
        Initialise le mod√®le Transformer.

        Args:
            num_items (int): Nombre total de produits
            embedding_dim (int): Dimension des embeddings
            seq_length (int): Longueur maximale des s√©quences
            num_heads (int): Nombre de t√™tes d'attention
            num_layers (int): Nombre de couches d'encodeur
            dropout (float): Taux de dropout pour la r√©gularisation
            feedforward_dim (int, optional): Dimension du feedforward (d√©faut: 2 * embedding_dim)
        """
        super().__init__()

        # Configuration du mod√®le
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.seq_length = seq_length
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout = dropout
        self.feedforward_dim = feedforward_dim or (2 * embedding_dim)

        # Validation des param√®tres
        if embedding_dim % num_heads != 0:
            raise ValueError(
                f"embedding_dim ({embedding_dim}) doit √™tre divisible par num_heads ({num_heads})"
            )

        # Couche d'embedding pour les produits
        # padding_idx=0 pour que le token de padding ait un embedding fixe √† z√©ro
        self.item_embedding = nn.Embedding(
            num_items + 1,  # +1 pour le token de padding
            embedding_dim,
            padding_idx=0,
        )

        # Architecture Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,  # Dimension du mod√®le
            nhead=num_heads,  # Nombre de t√™tes d'attention
            dim_feedforward=self.feedforward_dim,  # Dimension du feedforward
            dropout=dropout,  # Dropout pour la r√©gularisation
            activation="relu",  # Fonction d'activation
            batch_first=True,  # Format [batch, seq, features]
        )

        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Couche de normalisation finale
        self.layer_norm = nn.LayerNorm(embedding_dim)

        # Couche de sortie pour pr√©dire le prochain produit
        self.output_layer = nn.Linear(embedding_dim, num_items + 1)

        # Initialisation des poids
        self._init_weights()

        # Afficher la configuration
        self._log_model_info()

    def _init_weights(self):
        """Initialise les poids du mod√®le avec des valeurs appropri√©es."""
        # Initialisation Xavier pour les embeddings
        nn.init.xavier_uniform_(self.item_embedding.weight)

        # S'assurer que le padding embedding reste √† z√©ro
        with torch.no_grad():
            self.item_embedding.weight[0].fill_(0)

        # Initialisation Xavier pour la couche de sortie
        nn.init.xavier_uniform_(self.output_layer.weight)
        nn.init.zeros_(self.output_layer.bias)

    def _log_model_info(self):
        """Affiche les informations sur le mod√®le."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        logger.info("ü§ñ Mod√®le Transformer initialis√©:")
        logger.info(f"  ‚Ä¢ Produits: {self.num_items}")
        logger.info(f"  ‚Ä¢ Embedding dimension: {self.embedding_dim}")
        logger.info(f"  ‚Ä¢ Longueur s√©quences: {self.seq_length}")
        logger.info(f"  ‚Ä¢ T√™tes d'attention: {self.num_heads}")
        logger.info(f"  ‚Ä¢ Couches encodeur: {self.num_layers}")
        logger.info(f"  ‚Ä¢ Dropout: {self.dropout}")
        logger.info(f"  ‚Ä¢ Param√®tres totaux: {total_params:,}")
        logger.info(f"  ‚Ä¢ Param√®tres entra√Ænables: {trainable_params:,}")

    def forward(
        self, item_sequence: torch.Tensor, mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass du mod√®le.

        Args:
            item_sequence (torch.Tensor): S√©quence d'items [batch_size, seq_length]
            mask (torch.Tensor, optional): Masque d'attention pour ignorer le padding

        Returns:
            torch.Tensor: Logits pour la pr√©diction du prochain item [batch_size, num_items+1]
        """
        batch_size, seq_len = item_sequence.shape

        # √âtape 1: Convertir les IDs en embeddings
        embeddings = self.item_embedding(item_sequence)  # [batch, seq_len, embed_dim]

        # √âtape 2: Cr√©er un masque pour ignorer les tokens de padding si non fourni
        if mask is None:
            # Cr√©er un masque bool√©en: True pour les positions √† ignorer (padding)
            mask = item_sequence == 0  # [batch, seq_len]

        # √âtape 3: Passer par le Transformer
        # Note: PyTorch Transformer attend src_key_padding_mask
        sequence_output = self.transformer(
            embeddings, src_key_padding_mask=mask
        )  # [batch, seq_len, embed_dim]

        # √âtape 4: Normalisation
        sequence_output = self.layer_norm(sequence_output)

        # √âtape 5: Utiliser la repr√©sentation du dernier token non-padding
        # Pour chaque s√©quence, trouver le dernier token non-padding
        if mask is not None:
            # Trouver les indices des derniers tokens valides
            valid_lengths = (~mask).sum(dim=1) - 1  # -1 car indexation base 0
            valid_lengths = torch.clamp(valid_lengths, min=0)

            # Extraire les repr√©sentations des derniers tokens valides
            batch_indices = torch.arange(batch_size, device=item_sequence.device)
            last_outputs = sequence_output[
                batch_indices, valid_lengths
            ]  # [batch, embed_dim]
        else:
            # Si pas de masque, utiliser simplement le dernier token
            last_outputs = sequence_output[:, -1, :]  # [batch, embed_dim]

        # √âtape 6: Pr√©dire le prochain item
        logits = self.output_layer(last_outputs)  # [batch, num_items+1]

        return logits

    def predict_next_items(self, item_sequence: torch.Tensor, top_k: int = 5) -> tuple:
        """
        Pr√©dit les top-K prochains items pour une s√©quence.

        Args:
            item_sequence (torch.Tensor): S√©quence d'items
            top_k (int): Nombre de pr√©dictions √† retourner

        Returns:
            tuple: (predicted_items, probabilities)
        """
        self.eval()  # Mode √©valuation

        with torch.no_grad():
            # Forward pass
            logits = self.forward(item_sequence)

            # Convertir en probabilit√©s
            probabilities = torch.softmax(logits, dim=-1)

            # Top-K pr√©dictions
            top_probs, top_indices = torch.topk(probabilities, k=top_k, dim=-1)

            return top_indices, top_probs

    def get_embeddings(self, item_ids: torch.Tensor) -> torch.Tensor:
        """
        Retourne les embeddings pour des items donn√©s.

        Args:
            item_ids (torch.Tensor): IDs des items

        Returns:
            torch.Tensor: Embeddings correspondants
        """
        return self.item_embedding(item_ids)

    def save_model(self, path: str, additional_info: dict = None):
        """
        Sauvegarde le mod√®le avec sa configuration.

        Args:
            path (str): Chemin de sauvegarde
            additional_info (dict): Informations suppl√©mentaires √† sauvegarder
        """
        save_dict = {
            "model_state_dict": self.state_dict(),
            "model_config": {
                "num_items": self.num_items,
                "embedding_dim": self.embedding_dim,
                "seq_length": self.seq_length,
                "num_heads": self.num_heads,
                "num_layers": self.num_layers,
                "dropout": self.dropout,
                "feedforward_dim": self.feedforward_dim,
            },
        }

        if additional_info:
            save_dict.update(additional_info)

        torch.save(save_dict, path)
        logger.info(f"üíæ Mod√®le sauvegard√©: {path}")

    @classmethod
    def load_model(cls, path: str, device: str = "cpu"):
        """
        Charge un mod√®le depuis un fichier.

        Args:
            path (str): Chemin du mod√®le sauvegard√©
            device (str): Device sur lequel charger le mod√®le

        Returns:
            TransformerRecommendationModel: Mod√®le charg√©
        """
        checkpoint = torch.load(path, map_location=device)
        config = checkpoint["model_config"]

        # Cr√©er le mod√®le avec la configuration sauvegard√©e
        model = cls(**config)
        model.load_state_dict(checkpoint["model_state_dict"])
        model.to(device)

        logger.info(f"‚úÖ Mod√®le charg√© depuis: {path}")

        return model, checkpoint


def main():
    """Fonction principale pour test du module."""
    print("=" * 60)
    print("ü§ñ TEST DU MOD√àLE TRANSFORMER")
    print("=" * 60)

    # Configuration du test
    num_items = 50
    batch_size = 8
    seq_length = 10

    # Cr√©er le mod√®le
    model = TransformerRecommendationModel(
        num_items=num_items,
        embedding_dim=64,
        seq_length=seq_length,
        num_heads=4,
        num_layers=2,
        dropout=0.1,
    )

    # Cr√©er des donn√©es de test
    test_sequences = torch.randint(1, num_items + 1, (batch_size, seq_length))
    # Ajouter quelques tokens de padding
    test_sequences[:, -2:] = 0  # Padding sur les 2 derni√®res positions

    print(f"\nüìä Test avec {batch_size} s√©quences:")
    print(f"  ‚Ä¢ Forme input: {test_sequences.shape}")
    print(f"  ‚Ä¢ Exemple s√©quence: {test_sequences[0].tolist()}")

    # Forward pass
    with torch.no_grad():
        logits = model(test_sequences)
        print(f"  ‚Ä¢ Forme output: {logits.shape}")

        # Test de pr√©diction
        top_items, top_probs = model.predict_next_items(test_sequences[:1], top_k=5)
        print(f"  ‚Ä¢ Top 5 pr√©dictions: {top_items[0].tolist()}")
        print(f"  ‚Ä¢ Probabilit√©s: {top_probs[0].tolist()}")

    # Test de sauvegarde/chargement
    test_path = "test_model.pt"
    model.save_model(test_path, {"test_info": "Model test successful"})

    loaded_model, checkpoint = TransformerRecommendationModel.load_model(test_path)
    print(f"\n‚úÖ Test de sauvegarde/chargement r√©ussi!")

    # Nettoyer
    import os

    os.remove(test_path)

    print("üéâ Tous les tests sont pass√©s avec succ√®s!")


if __name__ == "__main__":
    main()
