"""
MODULE: ENTRA√éNEUR DE MOD√àLE
===========================

Ce module g√®re l'entra√Ænement du mod√®le Transformer pour
la recommandation de produits.

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import logging
import time
import os
from typing import Dict, List, Tuple, Optional

from .transformer_model import TransformerRecommendationModel

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainer:
    """
    Entra√Æneur pour le mod√®le Transformer de recommandation.

    Cette classe g√®re:
    - L'entra√Ænement avec validation
    - Le monitoring des m√©triques
    - La sauvegarde automatique
    - L'early stopping
    """

    def __init__(
        self,
        model: TransformerRecommendationModel,
        device: str = "auto",
        learning_rate: float = 1e-3,
        weight_decay: float = 1e-5,
        batch_size: int = 64,
    ):
        """
        Initialise l'entra√Æneur.

        Args:
            model: Mod√®le √† entra√Æner
            device: Device d'entra√Ænement ('auto', 'cpu', 'cuda')
            learning_rate: Taux d'apprentissage
            weight_decay: D√©croissance des poids
            batch_size: Taille des batches
        """
        self.model = model
        self.batch_size = batch_size

        # Configuration du device
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        self.model.to(self.device)

        # Configuration de l'optimiseur
        self.optimizer = optim.Adam(
            self.model.parameters(), lr=learning_rate, weight_decay=weight_decay
        )

        # Configuration de la loss fonction
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignorer le padding

        # Historique d'entra√Ænement
        self.training_history = {
            "train_loss": [],
            "val_loss": [],
            "learning_rates": [],
            "epochs": [],
        }

        logger.info("üöÄ Entra√Æneur initialis√©:")
        logger.info(f"  ‚Ä¢ Device: {self.device}")
        logger.info(f"  ‚Ä¢ Learning rate: {learning_rate}")
        logger.info(f"  ‚Ä¢ Batch size: {batch_size}")
        logger.info(
            f"  ‚Ä¢ Param√®tres mod√®le: {sum(p.numel() for p in model.parameters()):,}"
        )

    def create_dataloaders(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        val_split: float = 0.2,
        shuffle: bool = True,
    ) -> Tuple[DataLoader, DataLoader]:
        """
        Cr√©e les DataLoaders pour l'entra√Ænement et la validation.

        Args:
            inputs: Tenseur d'entr√©e
            targets: Tenseur de sortie
            val_split: Proportion des donn√©es pour la validation
            shuffle: M√©langer les donn√©es

        Returns:
            tuple: (train_loader, val_loader)
        """
        # D√©placer les donn√©es sur le bon device
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)

        # Division train/val
        n_samples = len(inputs)
        n_val = int(n_samples * val_split)
        n_train = n_samples - n_val

        if shuffle:
            # M√©langer les indices
            indices = torch.randperm(n_samples)
            train_indices = indices[:n_train]
            val_indices = indices[n_train:]
        else:
            train_indices = torch.arange(n_train)
            val_indices = torch.arange(n_train, n_samples)

        # Cr√©er les datasets
        train_dataset = TensorDataset(inputs[train_indices], targets[train_indices])
        val_dataset = TensorDataset(inputs[val_indices], targets[val_indices])

        # Cr√©er les DataLoaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            pin_memory=True if self.device.type == "cuda" else False,
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            pin_memory=True if self.device.type == "cuda" else False,
        )

        logger.info(f"üìä DataLoaders cr√©√©s:")
        logger.info(
            f"  ‚Ä¢ Train: {len(train_dataset):,} √©chantillons, {len(train_loader)} batches"
        )
        logger.info(
            f"  ‚Ä¢ Validation: {len(val_dataset):,} √©chantillons, {len(val_loader)} batches"
        )

        return train_loader, val_loader

    def train_epoch(self, train_loader: DataLoader) -> float:
        """
        Entra√Æne le mod√®le pour une √©poque.

        Args:
            train_loader: DataLoader d'entra√Ænement

        Returns:
            float: Loss moyenne de l'√©poque
        """
        self.model.train()
        total_loss = 0
        num_batches = 0

        for batch_inputs, batch_targets in train_loader:
            batch_inputs = batch_inputs.to(self.device)
            batch_targets = batch_targets.to(self.device)

            self.optimizer.zero_grad()

            # Training avec pr√©diction s√©quentielle
            batch_size_actual, seq_len = batch_inputs.shape
            all_losses = []

            # Pr√©dire √† chaque position de la s√©quence
            for pos in range(seq_len):
                if pos == 0:
                    continue  # Pas de pr√©diction sans contexte

                # Utiliser la s√©quence jusqu'√† la position pos
                input_seq = batch_inputs[:, : pos + 1]

                # Padding si n√©cessaire
                if input_seq.size(1) < self.model.seq_length:
                    padding = torch.zeros(
                        batch_size_actual,
                        self.model.seq_length - input_seq.size(1),
                        device=self.device,
                        dtype=torch.long,
                    )
                    input_seq = torch.cat([padding, input_seq], dim=1)

                # Forward pass
                logits = self.model(input_seq)
                target_items = batch_targets[:, pos - 1]

                # Loss seulement pour les tokens non-padding
                mask = target_items != 0
                if mask.sum() > 0:
                    loss = self.criterion(logits[mask], target_items[mask])
                    all_losses.append(loss)

            # Backpropagation
            if all_losses:
                batch_loss = torch.stack(all_losses).mean()
                batch_loss.backward()

                # Gradient clipping pour √©viter l'explosion des gradients
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                total_loss += batch_loss.item()
                num_batches += 1

        return total_loss / num_batches if num_batches > 0 else 0

    def validate_epoch(self, val_loader: DataLoader) -> float:
        """
        Valide le mod√®le pour une √©poque.

        Args:
            val_loader: DataLoader de validation

        Returns:
            float: Loss moyenne de validation
        """
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch_inputs, batch_targets in val_loader:
                batch_inputs = batch_inputs.to(self.device)
                batch_targets = batch_targets.to(self.device)

                # Validation avec la m√™me logique que l'entra√Ænement
                batch_size_actual, seq_len = batch_inputs.shape
                all_losses = []

                for pos in range(seq_len):
                    if pos == 0:
                        continue

                    input_seq = batch_inputs[:, : pos + 1]

                    if input_seq.size(1) < self.model.seq_length:
                        padding = torch.zeros(
                            batch_size_actual,
                            self.model.seq_length - input_seq.size(1),
                            device=self.device,
                            dtype=torch.long,
                        )
                        input_seq = torch.cat([padding, input_seq], dim=1)

                    logits = self.model(input_seq)
                    target_items = batch_targets[:, pos - 1]

                    mask = target_items != 0
                    if mask.sum() > 0:
                        loss = self.criterion(logits[mask], target_items[mask])
                        all_losses.append(loss)

                if all_losses:
                    batch_loss = torch.stack(all_losses).mean()
                    total_loss += batch_loss.item()
                    num_batches += 1

        return total_loss / num_batches if num_batches > 0 else 0

    def train(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        num_epochs: int = 10,
        val_split: float = 0.2,
        save_path: Optional[str] = None,
        save_best: bool = True,
        early_stopping_patience: int = 10,
        verbose: bool = True,
    ) -> Dict:
        """
        Entra√Æne le mod√®le.

        Args:
            inputs: Donn√©es d'entr√©e
            targets: Donn√©es cibles
            num_epochs: Nombre d'√©poques
            val_split: Proportion pour la validation
            save_path: Chemin de sauvegarde du mod√®le
            save_best: Sauvegarder le meilleur mod√®le
            early_stopping_patience: Patience pour l'early stopping
            verbose: Affichage d√©taill√©

        Returns:
            dict: Historique d'entra√Ænement
        """
        logger.info("üöÄ D√©but de l'entra√Ænement...")

        # Cr√©er les DataLoaders
        train_loader, val_loader = self.create_dataloaders(
            inputs, targets, val_split=val_split
        )

        # Variables pour l'early stopping
        best_val_loss = float("inf")
        patience_counter = 0

        start_time = time.time()

        for epoch in range(num_epochs):
            epoch_start_time = time.time()

            # Entra√Ænement
            train_loss = self.train_epoch(train_loader)

            # Validation
            val_loss = self.validate_epoch(val_loader)

            # Sauvegarder l'historique
            self.training_history["train_loss"].append(train_loss)
            self.training_history["val_loss"].append(val_loss)
            self.training_history["learning_rates"].append(
                self.optimizer.param_groups[0]["lr"]
            )
            self.training_history["epochs"].append(epoch + 1)

            # Temps d'√©poque
            epoch_time = time.time() - epoch_start_time

            if verbose:
                logger.info(
                    f"√âpoque {epoch + 1}/{num_epochs} | "
                    f"Train Loss: {train_loss:.4f} | "
                    f"Val Loss: {val_loss:.4f} | "
                    f"Temps: {epoch_time:.1f}s"
                )

            # Early stopping et sauvegarde du meilleur mod√®le
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0

                if save_best and save_path:
                    self.save_model(
                        save_path,
                        additional_info={
                            "epoch": epoch + 1,
                            "train_loss": train_loss,
                            "val_loss": val_loss,
                            "training_history": self.training_history,
                        },
                    )
            else:
                patience_counter += 1

                if patience_counter >= early_stopping_patience:
                    logger.info(f"‚è∞ Early stopping apr√®s {epoch + 1} √©poques")
                    break

        total_time = time.time() - start_time

        logger.info("‚úÖ Entra√Ænement termin√©!")
        logger.info(f"  ‚Ä¢ Temps total: {total_time:.1f}s")
        logger.info(f"  ‚Ä¢ Meilleure val loss: {best_val_loss:.4f}")
        logger.info(f"  ‚Ä¢ Loss finale train: {train_loss:.4f}")

        return self.training_history

    def save_model(self, path: str, additional_info: dict = None):
        """
        Sauvegarde le mod√®le et l'entra√Æneur.

        Args:
            path: Chemin de sauvegarde
            additional_info: Informations suppl√©mentaires
        """
        save_dict = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "training_history": self.training_history,
            "model_config": {
                "num_items": self.model.num_items,
                "embedding_dim": self.model.embedding_dim,
                "seq_length": self.model.seq_length,
                "num_heads": self.model.num_heads,
                "num_layers": self.model.num_layers,
                "dropout": self.model.dropout,
                "feedforward_dim": self.model.feedforward_dim,
            },
        }

        if additional_info:
            save_dict.update(additional_info)

        # Cr√©er le r√©pertoire si n√©cessaire
        os.makedirs(os.path.dirname(path), exist_ok=True)

        torch.save(save_dict, path)
        logger.info(f"üíæ Mod√®le et entra√Æneur sauvegard√©s: {path}")

    def load_checkpoint(self, path: str):
        """
        Charge un checkpoint d'entra√Ænement.

        Args:
            path: Chemin du checkpoint
        """
        checkpoint = torch.load(path, map_location=self.device)

        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        if "training_history" in checkpoint:
            self.training_history = checkpoint["training_history"]

        logger.info(f"‚úÖ Checkpoint charg√©: {path}")

        return checkpoint


def load_training_data(data_path: str):
    """
    Charge les donn√©es d'entra√Ænement depuis un fichier.

    Args:
        data_path: Chemin vers les donn√©es

    Returns:
        tuple: (inputs, targets)
    """
    logger.info(f"üìÇ Chargement des donn√©es d'entra√Ænement: {data_path}")

    data = torch.load(data_path, map_location="cpu")
    inputs = data["inputs"]
    targets = data["targets"]

    logger.info(f"‚úÖ Donn√©es charg√©es:")
    logger.info(f"  ‚Ä¢ Inputs: {inputs.shape}")
    logger.info(f"  ‚Ä¢ Targets: {targets.shape}")

    return inputs, targets


def main():
    """Fonction principale pour test du module."""
    print("=" * 60)
    print("üöÄ TEST DE L'ENTRA√éNEMENT")
    print("=" * 60)

    # Cr√©er un mod√®le de test
    model = TransformerRecommendationModel(
        num_items=50, embedding_dim=64, seq_length=10, num_heads=4, num_layers=2
    )

    # Cr√©er des donn√©es de test
    batch_size = 100
    seq_length = 9  # -1 car inputs sont plus courts que sequences
    inputs = torch.randint(1, 51, (batch_size, seq_length))
    targets = torch.randint(1, 51, (batch_size, seq_length))

    # Cr√©er l'entra√Æneur
    trainer = ModelTrainer(
        model=model,
        device="cpu",  # Force CPU pour le test
        learning_rate=1e-3,
        batch_size=32,
    )

    # Entra√Æner le mod√®le
    history = trainer.train(
        inputs=inputs,
        targets=targets,
        num_epochs=3,
        val_split=0.2,
        save_path="models/test_model.pt",
        verbose=True,
    )

    print(f"\n‚úÖ Test d'entra√Ænement termin√©!")
    print(f"  ‚Ä¢ √âpoques: {len(history['train_loss'])}")
    print(f"  ‚Ä¢ Loss finale train: {history['train_loss'][-1]:.4f}")
    print(f"  ‚Ä¢ Loss finale val: {history['val_loss'][-1]:.4f}")

    # Nettoyer
    if os.path.exists("models/test_model.pt"):
        os.remove("models/test_model.pt")
        os.rmdir("models")


if __name__ == "__main__":
    main()
