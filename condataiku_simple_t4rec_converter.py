"""
S√âLECTION INTELLIGENTE DE COLONNES
==================================================

Script avec plusieurs strat√©gies de s√©lection intelligente :
1. Analyse de variance (features discriminantes)
2. Corr√©lation et redondance
3. S√©lection m√©tier bancaire
4. Analyse d'importance via Random Forest
5. Analyse temporelle et patterns

Choisit automatiquement les meilleures colonnes selon plusieurs crit√®res.
"""

import dataiku
import numpy as np
import pandas as pd
import json
import torch
from typing import List, Tuple, Dict, Any
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import re


def load_input_dataset() -> pd.DataFrame:
    """Charge le dataset d'input depuis Dataiku"""
    try:
        dataset = dataiku.Dataset("t4_rec_df")
        df = dataset.get_dataframe()
        print(f"Dataset input 't4_rec_df' charg√©: {df.shape}")
        return df
    except Exception as e:
        print(f"Erreur chargement input: {e}")
        raise


def detect_and_convert_arrays(df: pd.DataFrame) -> pd.DataFrame:
    """D√©tecte et convertit les colonnes contenant des arrays JSON"""
    print(f"üîç Analyse de {len(df.columns)} colonnes...")
    df_converted = df.copy()
    converted_count = 0

    for col in df.columns:
        if df[col].dtype == "object":
            sample_values = df[col].dropna().head(3).tolist()

            is_json_array = False
            for val in sample_values:
                if (
                    isinstance(val, str)
                    and val.strip().startswith("[")
                    and val.strip().endswith("]")
                ):
                    is_json_array = True
                    break

            if is_json_array:

                def convert_json_array(val):
                    try:
                        if pd.isna(val):
                            return np.nan
                        if isinstance(val, str):
                            array_val = json.loads(val)
                            if isinstance(array_val, list) and len(array_val) > 0:
                                return float(array_val[0])
                        return float(val) if val is not None else np.nan
                    except:
                        return np.nan

                df_converted[col] = df[col].apply(convert_json_array)
                converted_count += 1

    print(f"üìà {converted_count} colonnes arrays converties")
    return df_converted


class SmartColumnSelector:
    """Classe pour s√©lection intelligente de colonnes"""

    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        print(
            f" SmartSelector initialis√© avec {len(self.numeric_cols)} colonnes num√©riques"
        )

    def analyze_variance(self, threshold: float = 0.01) -> List[str]:
        """1. ANALYSE DE VARIANCE - √âlimine les features constantes"""
        print(f" Analyse de variance (seuil: {threshold})...")

        # Calculer la variance de chaque colonne
        variances = self.df[self.numeric_cols].var().sort_values(ascending=False)
        high_variance_cols = variances[variances > threshold].index.tolist()

        print(
            f"  {len(high_variance_cols)}/{len(self.numeric_cols)} colonnes avec variance > {threshold}"
        )
        print(f"  üìà Top 5 variances: {variances.head().to_dict()}")

        return high_variance_cols

    def analyze_correlation(
        self, features: List[str], threshold: float = 0.95
    ) -> List[str]:
        """2. ANALYSE CORR√âLATION - √âlimine les features redondantes"""
        print(f" 2. Analyse corr√©lation (seuil: {threshold})...")

        if len(features) < 2:
            return features

        # Matrice de corr√©lation
        corr_matrix = self.df[features].corr().abs()

        # Trouver les paires tr√®s corr√©l√©es
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )

        # Colonnes √† supprimer (gardant celle avec plus de variance)
        to_drop = set()
        for col in upper_tri.columns:
            highly_corr = upper_tri[col][upper_tri[col] > threshold].index.tolist()
            if highly_corr:
                # Garder la colonne avec plus de variance
                variances = self.df[[col] + highly_corr].var()
                to_drop.update(variances.drop(variances.idxmax()).index.tolist())

        selected_features = [col for col in features if col not in to_drop]

        print(
            f"  {len(selected_features)}/{len(features)} colonnes apr√®s suppression corr√©lation"
        )
        print(f"   Supprim√©es pour corr√©lation > {threshold}: {len(to_drop)} colonnes")

        return selected_features

    def banking_domain_selection(self, features: List[str]) -> Dict[str, List[str]]:
        """3. S√âLECTION M√âTIER BANCAIRE - Logique business"""
        print(f"üè¶ 3. S√©lection m√©tier bancaire...")

        banking_categories = {
            "temporal_evolution": [],  # Evolution temporelle (3m, 6m, 9m, 12m)
            "transaction_volume": [],  # Volumes de transactions
            "transaction_amounts": [],  # Montants
            "product_usage": [],  # Usage produits bancaires
            "demographics": [],  # D√©mographie client
            "risk_indicators": [],  # Indicateurs de risque
            "digital_behavior": [],  # Comportement digital
            "geographical": [],  # G√©ographie
        }

        # Patterns pour classification automatique
        patterns = {
            "temporal_evolution": [
                r"_3m$",
                r"_6m$",
                r"_9m$",
                r"_12m$",
                r"_3dm$",
                r"_6dm$",
                r"_12dm$",
            ],
            "transaction_volume": [r"^nb_", r"^top_", r"_nb_", r"nbr"],
            "transaction_amounts": [r"^mnt", r"^mt_", r"somme_", r"var_mnt"],
            "product_usage": [
                r"livret",
                r"euro",
                r"carte",
                r"credit",
                r"epargne",
                r"assur",
            ],
            "demographics": [r"age", r"enfant", r"iris", r"pop_", r"csp"],
            "risk_indicators": [r"decou", r"resil", r"fermeture", r"enc_"],
            "digital_behavior": [r"mobile", r"contact", r"visite", r"services"],
            "geographical": [r"iris", r"region", r"zone"],
        }

        # Classifier chaque feature
        for feature in features:
            feature_lower = feature.lower()
            categorized = False

            for category, pattern_list in patterns.items():
                for pattern in pattern_list:
                    if re.search(pattern, feature_lower):
                        banking_categories[category].append(feature)
                        categorized = True
                        break
                if categorized:
                    break

        # Afficher la classification
        for category, cols in banking_categories.items():
            if cols:
                print(f"  üìÇ {category}: {len(cols)} colonnes")

        return banking_categories

    def importance_based_selection(
        self, features: List[str], top_k: int = 50
    ) -> List[str]:
        """4. S√âLECTION PAR IMPORTANCE - Random Forest"""
        print(f" S√©lection par importance Random Forest (top {top_k})...")

        if len(features) <= top_k:
            return features

        try:
            # Cr√©er une target synth√©tique (somme de quelques colonnes importantes)
            target_cols = [
                col
                for col in features
                if any(x in col.lower() for x in ["mnt", "somme", "nb_"])
            ][:5]
            if target_cols:
                y = self.df[target_cols].sum(axis=1)
            else:
                y = self.df[features].sum(axis=1)

            # Pr√©parer les donn√©es
            X = self.df[features].fillna(0)

            # Random Forest pour importance
            rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
            rf.fit(X, y)

            # R√©cup√©rer l'importance
            feature_importance = pd.DataFrame(
                {"feature": features, "importance": rf.feature_importances_}
            ).sort_values("importance", ascending=False)

            selected_features = feature_importance.head(top_k)["feature"].tolist()

            print(
                f"  Top {len(selected_features)} features s√©lectionn√©es par importance"
            )
            print(f"  Top 5: {selected_features[:5]}")

            return selected_features

        except Exception as e:
            print(f"  Erreur Random Forest: {e}, utilisation variance √† la place")
            variances = self.df[features].var().sort_values(ascending=False)
            return variances.head(top_k).index.tolist()

    def temporal_pattern_analysis(self, features: List[str]) -> List[str]:
        """5. ANALYSE PATTERNS TEMPORELS - S√©lectionne selon √©volution temporelle"""
        print(f"Analyse patterns temporels...")

        # Identifier les colonnes temporelles
        temporal_groups = {}
        for feature in features:
            # Extraire le nom de base (sans suffixe temporel)
            base_name = re.sub(r"_(3m|6m|9m|12m|3dm|6dm|12dm)$", "", feature.lower())
            if base_name != feature.lower():  # A un suffixe temporel
                if base_name not in temporal_groups:
                    temporal_groups[base_name] = []
                temporal_groups[base_name].append(feature)

        # Pour chaque groupe temporel, s√©lectionner la p√©riode la plus r√©cente et la plus ancienne
        selected_temporal = []
        for base_name, group in temporal_groups.items():
            if len(group) > 1:
                # Ordre de pr√©f√©rence : 3m (plus r√©cent), puis 12m (plus de contexte)
                priority_order = ["3m", "6m", "12m", "3dm", "6dm", "12dm"]
                for period in priority_order:
                    matching = [col for col in group if col.lower().endswith(period)]
                    if matching:
                        selected_temporal.extend(matching[:1])  # Prendre le premier
                        break
            else:
                selected_temporal.extend(group)

        # Ajouter les colonnes non-temporelles
        non_temporal = [
            f for f in features if f not in sum(temporal_groups.values(), [])
        ]
        final_selection = selected_temporal + non_temporal

        print(f" {len(temporal_groups)} groupes temporels analys√©s")
        print(f"  {len(selected_temporal)} colonnes temporelles optimis√©es")
        print(f" {len(final_selection)} colonnes finales")

        return final_selection

    def smart_selection_pipeline(
        self,
        max_features: int = 30,
        variance_threshold: float = 0.01,
        correlation_threshold: float = 0.95,
    ) -> Tuple[List[str], Dict]:
        """PIPELINE COMPLET de s√©lection intelligente"""
        print(f"\n PIPELINE S√âLECTION INTELLIGENTE")
        print(f" Objectif: {max_features} features optimales")
        print("=" * 50)

        stats = {}

        # √âtape 1: Variance
        high_var_features = self.analyze_variance(variance_threshold)
        stats["step1_variance"] = len(high_var_features)

        # √âtape 2: Corr√©lation
        low_corr_features = self.analyze_correlation(
            high_var_features, correlation_threshold
        )
        stats["step2_correlation"] = len(low_corr_features)

        # √âtape 3: Classification m√©tier
        banking_categories = self.banking_domain_selection(low_corr_features)
        stats["step3_banking_categories"] = banking_categories

        # √âtape 4: Patterns temporels
        temporal_optimized = self.temporal_pattern_analysis(low_corr_features)
        stats["step4_temporal"] = len(temporal_optimized)

        # √âtape 5: Importance (si encore trop de features)
        if len(temporal_optimized) > max_features:
            final_features = self.importance_based_selection(
                temporal_optimized, max_features
            )
        else:
            final_features = temporal_optimized

        stats["final_features_count"] = len(final_features)
        stats["final_features"] = final_features

        print(f"\n S√âLECTION TERMIN√âE!")
        print(
            f"Pipeline: {len(self.numeric_cols)} ‚Üí {stats['step1_variance']} ‚Üí {stats['step2_correlation']} ‚Üí {stats['step4_temporal']} ‚Üí {len(final_features)}"
        )
        print(f"Features finales: {len(final_features)}")

        return final_features, stats


def create_sequences_smart_selection(
    df: pd.DataFrame, selected_features: List[str], seq_length: int = None
) -> Tuple[np.ndarray, Dict]:
    """Cr√©e des s√©quences avec les colonnes s√©lectionn√©es intelligemment"""

    if seq_length is None:
        seq_length = min(len(selected_features), 30)  # Adaptatif

    print(f"Cr√©ation s√©quences avec {len(selected_features)} features s√©lectionn√©es")
    print(f"Longueur s√©quence: {seq_length}")

    sequences = []

    for idx, row in df.iterrows():
        # R√©cup√©rer les valeurs des features s√©lectionn√©es
        values = [row[col] for col in selected_features]

        # Filtrer les valeurs NaN/infinies
        values = [v for v in values if not (pd.isna(v) or np.isinf(v))]

        if len(values) < seq_length:
            # R√©p√©ter les valeurs si pas assez
            while len(values) < seq_length:
                values.extend(values[: min(len(values), seq_length - len(values))])
            values = values[:seq_length]
        else:
            # Prendre les premi√®res valeurs
            values = values[:seq_length]

        sequences.append(values)

        if idx % 1000 == 0:
            print(f" Trait√© {idx + 1} lignes...")

    sequences_array = np.array(sequences)
    print(f"S√©quences cr√©√©es: {sequences_array.shape}")

    stats = {
        "num_sequences": len(sequences),
        "sequence_length": seq_length,
        "selected_columns": selected_features,
        "selection_method": "smart_pipeline",
        "value_range": (float(np.min(sequences_array)), float(np.max(sequences_array))),
        "mean_value": float(np.mean(sequences_array)),
    }

    return sequences_array, stats


def create_t4rec_format(sequences: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
    """Convertit les s√©quences au format T4Rec"""
    print(f" Conversion au format T4Rec...")

    # Normaliser les valeurs entre 1 et 2000
    sequences_norm = sequences.copy()
    seq_min = np.min(sequences_norm)
    seq_max = np.max(sequences_norm)

    if seq_max != seq_min:
        sequences_norm = (sequences_norm - seq_min) / (seq_max - seq_min)
        sequences_norm = sequences_norm * 1999 + 1  # [1, 2000]
    else:
        sequences_norm = np.ones_like(sequences_norm)

    sequences_ids = sequences_norm.astype(int)

    # Cr√©er input/target pairs
    inputs = sequences_ids[:, :-1]
    targets = sequences_ids[:, 1:]

    input_tensor = torch.LongTensor(inputs)
    target_tensor = torch.LongTensor(targets)

    print(f"  Input shape: {input_tensor.shape}")
    print(f" Target shape: {target_tensor.shape}")
    print(
        f"  Item ID range: {int(np.min(sequences_ids))} - {int(np.max(sequences_ids))}"
    )

    return input_tensor, target_tensor


def save_selection_analysis(
    selection_stats: Dict, filename: str = "column_selection_analysis.txt"
):
    """Sauvegarde l'analyse de s√©lection"""

    analysis = f"""
# ANALYSE S√âLECTION INTELLIGENTE DE COLONNES
============================================

## Pipeline de s√©lection:
1. Analyse variance: {selection_stats.get("step1_variance", 0)} colonnes
2. Suppression corr√©lation: {selection_stats.get("step2_correlation", 0)} colonnes  
3. Optimisation temporelle: {selection_stats.get("step4_temporal", 0)} colonnes
4. S√©lection finale: {selection_stats.get("final_features_count", 0)} colonnes

## Cat√©gories m√©tier identifi√©es:
"""

    banking_cats = selection_stats.get("step3_banking_categories", {})
    for category, cols in banking_cats.items():
        if cols:
            analysis += f"- {category}: {len(cols)} colonnes\n"

    analysis += f"""
## Colonnes finales s√©lectionn√©es:
{chr(10).join(f"- {col}" for col in selection_stats.get("final_features", []))}

## Avantages de cette s√©lection:
Variance √©lev√©e (features discriminantes)
Faible corr√©lation (pas de redondance)  
Logique m√©tier bancaire respect√©e
Optimisation patterns temporels
S√©lection par importance statistique
"""

    with open(filename, "w", encoding="utf-8") as f:
        f.write(analysis)

    print(f"Analyse sauvegard√©e: {filename}")


def main():
    """Pipeline principal - S√âLECTION INTELLIGENTE"""
    print(" D√âMARRAGE RECIPE S√âLECTION INTELLIGENTE")
    print("=" * 60)

    try:
        # 1. Charger et convertir
        df = load_input_dataset()
        df_converted = detect_and_convert_arrays(df)

        # 2. S√©lection intelligente
        selector = SmartColumnSelector(df_converted)
        selected_features, selection_stats = selector.smart_selection_pipeline(
            max_features=30,  # Nombre optimal de features
            variance_threshold=0.01,
            correlation_threshold=0.95,
        )

        # 3. Cr√©er les s√©quences avec features s√©lectionn√©es
        sequences, stats = create_sequences_smart_selection(
            df_converted,
            selected_features,
            seq_length=25,  # S√©quence adapt√©e
        )

        # 4. Format T4Rec
        inputs, targets = create_t4rec_format(sequences)

        # 5. Sauvegardes
        stats["num_unique_items"] = int(
            torch.max(torch.cat([inputs.flatten(), targets.flatten()])).item()
        )

        # Sauvegarde analyse
        save_selection_analysis(selection_stats)

        # Sauvegarde tenseurs
        data_dict = {
            "inputs": inputs,
            "targets": targets,
            "metadata": {
                "num_sequences": inputs.shape[0],
                "sequence_length": inputs.shape[1],
                "num_unique_items": stats["num_unique_items"],
                "selected_columns": selected_features,
                "selection_method": "smart_pipeline",
                "selection_stats": selection_stats,
            },
        }

        torch.save(data_dict, "t4rec_training_data_SMART_SELECTION.pt")

        print(f"\n S√âLECTION INTELLIGENTE TERMIN√âE!")
        print(f"{len(selected_features)} colonnes optimales s√©lectionn√©es")
        print(f"Donn√©es sauvegard√©es: t4rec_training_data_SMART_SELECTION.pt")
        print(f" Analyse d√©taill√©e: column_selection_analysis.txt")

        print(f"\n TOP FEATURES S√âLECTIONN√âES:")
        for i, feature in enumerate(selected_features[:10], 1):
            print(f"  {i:2d}. {feature}")

    except Exception as e:
        print(f"\n ERREUR: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
