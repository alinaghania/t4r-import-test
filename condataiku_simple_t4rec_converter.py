#!/usr/bin/env python3
"""
DATAIKU RECIPE - S√âLECTION INTELLIGENTE T4REC (SANS SKLEARN)
===========================================================

Version adapt√©e pour environnements sans sklearn.
Remplace Random Forest par analyse de corr√©lation avec target synth√©tique.

Auteur: Assistant IA
Version: 2.0 (No-sklearn)
"""

import json
import re
import numpy as np
import pandas as pd
import torch
from typing import List, Dict, Tuple
import dataiku
from dataiku import Dataset

print("üöÄ Import des modules termin√©")


def load_input_dataset() -> pd.DataFrame:
    """Charge le dataset d'entr√©e depuis Dataiku"""
    print("üì• Chargement dataset d'entr√©e...")

    # Dataset d'entr√©e configur√© dans Dataiku
    input_dataset = dataiku.Dataset("t4_rec_df")
    df = input_dataset.get_dataframe()

    print(f"‚úÖ Dataset charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
    return df


def detect_and_convert_arrays(df: pd.DataFrame) -> pd.DataFrame:
    """D√©tecte et convertit les colonnes contenant des arrays JSON"""
    print(f"üîç Analyse de {df.shape[1]} colonnes pour arrays JSON...")

    df_converted = df.copy()
    converted_count = 0

    for col in df.columns:
        if df[col].dtype == "object":
            sample_val = (
                df[col].dropna().iloc[0] if not df[col].dropna().empty else None
            )

            is_json_array = False
            if isinstance(sample_val, str):
                try:
                    parsed = json.loads(sample_val)
                    if isinstance(parsed, list):
                        is_json_array = True
                except:
                    pass

            if is_json_array:

                def convert_json_array(val):
                    try:
                        if pd.isna(val):
                            return np.nan
                        if isinstance(val, str):
                            array_val = json.loads(val)
                            if isinstance(array_val, list) and len(array_val) > 0:
                                return float(array_val[0])
                        return float(val) if val is not None else np.nan
                    except:
                        return np.nan

                df_converted[col] = df[col].apply(convert_json_array)
                converted_count += 1

    print(f"üìà {converted_count} colonnes arrays converties")
    return df_converted


class SmartColumnSelectorNoSklearn:
    """Classe pour s√©lection intelligente de colonnes SANS sklearn"""

    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        print(
            f"üéØ SmartSelector initialis√© avec {len(self.numeric_cols)} colonnes num√©riques"
        )

    def analyze_variance(self, threshold: float = 0.01) -> List[str]:
        """1. ANALYSE DE VARIANCE - √âlimine les features constantes"""
        print(f"üìä 1. Analyse de variance (seuil: {threshold})...")

        # Calculer la variance de chaque colonne
        variances = self.df[self.numeric_cols].var().sort_values(ascending=False)
        high_variance_cols = variances[variances > threshold].index.tolist()

        print(
            f"{len(high_variance_cols)}/{len(self.numeric_cols)} colonnes avec variance > {threshold}"
        )
        print(f"  üìà Top 5 variances: {variances.head().to_dict()}")

        return high_variance_cols

    def analyze_correlation(
        self, features: List[str], threshold: float = 0.95
    ) -> List[str]:
        """2. ANALYSE CORR√âLATION - √âlimine les features redondantes"""
        print(f"üîó 2. Analyse corr√©lation (seuil: {threshold})...")

        if len(features) < 2:
            return features

        # Matrice de corr√©lation
        corr_matrix = self.df[features].corr().abs()

        # Trouver les paires tr√®s corr√©l√©es
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )

        # Colonnes √† supprimer (gardant celle avec plus de variance)
        to_drop = set()
        for col in upper_tri.columns:
            highly_corr = upper_tri[col][upper_tri[col] > threshold].index.tolist()
            if highly_corr:
                # Garder la colonne avec plus de variance
                variances = self.df[[col] + highly_corr].var()
                to_drop.update(variances.drop(variances.idxmax()).index.tolist())

        selected_features = [col for col in features if col not in to_drop]

        print(
            f"  ‚úÖ {len(selected_features)}/{len(features)} colonnes apr√®s suppression corr√©lation"
        )
        print(f"  üóëÔ∏è Supprim√©es pour corr√©lation > {threshold}: {len(to_drop)} colonnes")

        return selected_features

    def banking_domain_selection(self, features: List[str]) -> Dict[str, List[str]]:
        """3. S√âLECTION M√âTIER BANCAIRE - Logique business"""
        print(f"üè¶ 3. S√©lection m√©tier bancaire...")

        banking_categories = {
            "temporal_evolution": [],  # Evolution temporelle (3m, 6m, 9m, 12m)
            "transaction_volume": [],  # Volumes de transactions
            "transaction_amounts": [],  # Montants
            "product_usage": [],  # Usage produits bancaires
            "demographics": [],  # D√©mographie client
            "risk_indicators": [],  # Indicateurs de risque
            "digital_behavior": [],  # Comportement digital
            "geographical": [],  # G√©ographie
        }

        # Patterns pour classification automatique
        patterns = {
            "temporal_evolution": [
                r"_3m$",
                r"_6m$",
                r"_9m$",
                r"_12m$",
                r"_3dm$",
                r"_6dm$",
                r"_12dm$",
            ],
            "transaction_volume": [r"^nb_", r"^top_", r"_nb_", r"nbr"],
            "transaction_amounts": [r"^mnt", r"^mt_", r"somme_", r"var_mnt"],
            "product_usage": [
                r"livret",
                r"euro",
                r"carte",
                r"credit",
                r"epargne",
                r"assur",
            ],
            "demographics": [r"age", r"enfant", r"iris", r"pop_", r"csp"],
            "risk_indicators": [r"decou", r"resil", r"fermeture", r"enc_"],
            "digital_behavior": [r"mobile", r"contact", r"visite", r"services"],
            "geographical": [r"iris", r"region", r"zone"],
        }

        # Classifier chaque feature
        for feature in features:
            feature_lower = feature.lower()
            categorized = False

            for category, pattern_list in patterns.items():
                for pattern in pattern_list:
                    if re.search(pattern, feature_lower):
                        banking_categories[category].append(feature)
                        categorized = True
                        break
                if categorized:
                    break

        # Afficher la classification
        for category, cols in banking_categories.items():
            if cols:
                print(f"  üìÇ {category}: {len(cols)} colonnes")

        return banking_categories

    def correlation_based_selection(
        self, features: List[str], top_k: int = 50
    ) -> List[str]:
        """4. S√âLECTION PAR CORR√âLATION AVEC TARGET (Remplace Random Forest)"""
        print(
            f"üìä 4. S√©lection par corr√©lation avec target synth√©tique (top {top_k})..."
        )

        if len(features) <= top_k:
            return features

        try:
            # Cr√©er une target synth√©tique intelligente
            target_cols = [
                col
                for col in features
                if any(x in col.lower() for x in ["mnt", "somme", "nb_"])
            ][:5]

            if len(target_cols) >= 2:
                # Target = combinaison de colonnes importantes
                y = self.df[target_cols].fillna(0).sum(axis=1)
            else:
                # Fallback: variance pond√©r√©e
                variances = self.df[features].var()
                weights = variances / variances.sum()
                y = (self.df[features].fillna(0) * weights).sum(axis=1)

            # Calculer corr√©lation absolue avec target
            correlations = []
            for feature in features:
                try:
                    corr = abs(self.df[feature].fillna(0).corr(y))
                    correlations.append((feature, corr if not pd.isna(corr) else 0))
                except:
                    correlations.append((feature, 0))

            # Trier par corr√©lation d√©croissante
            correlations.sort(key=lambda x: x[1], reverse=True)
            selected_features = [feat for feat, corr in correlations[:top_k]]

            print(
                f"  ‚úÖ Top {len(selected_features)} features s√©lectionn√©es par corr√©lation"
            )
            print(f"  üèÜ Top 5: {selected_features[:5]}")
            print(
                f"  üìä Corr√©lations top 5: {[round(corr, 3) for _, corr in correlations[:5]]}"
            )

            return selected_features

        except Exception as e:
            print(f"  ‚ö†Ô∏è Erreur corr√©lation: {e}, utilisation variance √† la place")
            variances = self.df[features].var().sort_values(ascending=False)
            return variances.head(top_k).index.tolist()

    def temporal_pattern_analysis(self, features: List[str]) -> List[str]:
        """5. ANALYSE PATTERNS TEMPORELS - S√©lectionne selon √©volution temporelle"""
        print(f"‚è∞ 5. Analyse patterns temporels...")

        # Identifier les colonnes temporelles
        temporal_groups = {}
        non_temporal = []

        for feature in features:
            # Extraire le nom de base (sans suffixe temporel)
            base_name = re.sub(r"_(3m|6m|9m|12m|3dm|6dm|12dm)$", "", feature.lower())

            if base_name != feature.lower():  # C'est une colonne temporelle
                if base_name not in temporal_groups:
                    temporal_groups[base_name] = []
                temporal_groups[base_name].append(feature)
            else:
                non_temporal.append(feature)

        # Pour chaque groupe temporel, s√©lectionner la meilleure p√©riode
        selected_temporal = []
        for base_name, temporal_cols in temporal_groups.items():
            if len(temporal_cols) > 1:
                # Pr√©f√©rer 12m > 9m > 6m > 3m (plus de donn√©es historiques)
                priority_order = ["12m", "9m", "6m", "3m", "12dm", "6dm", "3dm"]
                best_col = None

                for period in priority_order:
                    matching_cols = [
                        col for col in temporal_cols if period in col.lower()
                    ]
                    if matching_cols:
                        # Prendre celui avec la plus grande variance
                        variances = self.df[matching_cols].var()
                        best_col = variances.idxmax()
                        break

                if best_col:
                    selected_temporal.append(best_col)
                else:
                    # Fallback: prendre celui avec plus de variance
                    variances = self.df[temporal_cols].var()
                    selected_temporal.append(variances.idxmax())
            else:
                selected_temporal.extend(temporal_cols)

        final_features = selected_temporal + non_temporal

        print(f"  ‚è∞ {len(temporal_groups)} groupes temporels optimis√©s")
        print(f"  ‚úÖ {len(final_features)} features apr√®s optimisation temporelle")

        return final_features

    def smart_selection_pipeline(
        self,
        max_features: int = 30,
        variance_threshold: float = 0.01,
        correlation_threshold: float = 0.95,
    ) -> Tuple[List[str], Dict]:
        """PIPELINE COMPLET de s√©lection intelligente (SANS SKLEARN)"""
        print(f"\nüß† PIPELINE S√âLECTION INTELLIGENTE (NO-SKLEARN)")
        print(f"üéØ Objectif: {max_features} features optimales")
        print("=" * 50)

        stats = {}

        # √âtape 1: Variance
        high_var_features = self.analyze_variance(variance_threshold)
        stats["step1_variance"] = len(high_var_features)

        # √âtape 2: Corr√©lation
        low_corr_features = self.analyze_correlation(
            high_var_features, correlation_threshold
        )
        stats["step2_correlation"] = len(low_corr_features)

        # √âtape 3: Classification m√©tier
        banking_categories = self.banking_domain_selection(low_corr_features)
        stats["step3_banking_categories"] = banking_categories

        # √âtape 4: Patterns temporels
        temporal_optimized = self.temporal_pattern_analysis(low_corr_features)
        stats["step4_temporal"] = len(temporal_optimized)

        # √âtape 5: S√©lection par corr√©lation (au lieu de Random Forest)
        if len(temporal_optimized) > max_features:
            final_features = self.correlation_based_selection(
                temporal_optimized, max_features
            )
        else:
            final_features = temporal_optimized

        stats["final_features_count"] = len(final_features)
        stats["final_features"] = final_features

        print(f"\nüéâ S√âLECTION TERMIN√âE!")
        print(
            f"üìä Pipeline: {len(self.numeric_cols)} ‚Üí {stats['step1_variance']} ‚Üí {stats['step2_correlation']} ‚Üí {stats['step4_temporal']} ‚Üí {len(final_features)}"
        )
        print(f"‚úÖ Features finales: {len(final_features)}")

        return final_features, stats


def create_sequences_smart_selection(
    df: pd.DataFrame, selected_features: List[str], seq_length: int = None
) -> Tuple[np.ndarray, Dict]:
    """Cr√©e des s√©quences avec les colonnes s√©lectionn√©es intelligemment"""

    if seq_length is None:
        seq_length = min(len(selected_features), 30)  # Adaptatif

    print(f"üîÑ Cr√©ation s√©quences avec {len(selected_features)} features s√©lectionn√©es")
    print(f"üìè Longueur s√©quence: {seq_length}")

    sequences = []

    for idx, row in df.iterrows():
        # R√©cup√©rer les valeurs des features s√©lectionn√©es
        values = [row[col] for col in selected_features]

        # Filtrer les valeurs NaN/infinies
        values = [v for v in values if not (pd.isna(v) or np.isinf(v))]

        if len(values) < seq_length:
            # R√©p√©ter les valeurs si pas assez
            while len(values) < seq_length:
                values.extend(values[: min(len(values), seq_length - len(values))])
            values = values[:seq_length]
        else:
            # Prendre les premi√®res valeurs
            values = values[:seq_length]

        sequences.append(values)

        if idx % 1000 == 0:
            print(f"  üìù Trait√© {idx + 1} lignes...")

    sequences_array = np.array(sequences)
    print(f"‚úÖ S√©quences cr√©√©es: {sequences_array.shape}")

    stats = {
        "num_sequences": len(sequences),
        "sequence_length": seq_length,
        "selected_columns": selected_features,
        "selection_method": "smart_pipeline_no_sklearn",
        "value_range": (float(np.min(sequences_array)), float(np.max(sequences_array))),
        "mean_value": float(np.mean(sequences_array)),
    }

    return sequences_array, stats


def create_t4rec_format(sequences: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
    """Convertit les s√©quences au format T4Rec"""
    print(f"üéØ Conversion au format T4Rec...")

    # Normaliser les valeurs entre 1 et 2000
    sequences_norm = sequences.copy()
    seq_min = np.min(sequences_norm)
    seq_max = np.max(sequences_norm)

    if seq_max != seq_min:
        sequences_norm = (sequences_norm - seq_min) / (seq_max - seq_min)
        sequences_norm = sequences_norm * 1999 + 1  # [1, 2000]
    else:
        sequences_norm = np.ones_like(sequences_norm)

    sequences_ids = sequences_norm.astype(int)

    # Cr√©er input/target pairs
    inputs = sequences_ids[:, :-1]
    targets = sequences_ids[:, 1:]

    input_tensor = torch.LongTensor(inputs)
    target_tensor = torch.LongTensor(targets)

    print(f"  üìä Input shape: {input_tensor.shape}")
    print(f"  üìä Target shape: {target_tensor.shape}")
    print(
        f"  üìä Item ID range: {int(np.min(sequences_ids))} - {int(np.max(sequences_ids))}"
    )

    return input_tensor, target_tensor


def save_outputs(
    df_final: pd.DataFrame,
    input_tensor: torch.Tensor,
    target_tensor: torch.Tensor,
    selection_stats: Dict,
):
    """Sauvegarde les r√©sultats dans Dataiku"""
    print("üíæ Sauvegarde des r√©sultats...")

    # 1. Dataset CSV avec m√©tadonn√©es + s√©quences
    output_dataset = dataiku.Dataset("t4_rec_df_clean")
    output_dataset.write_with_schema(df_final)
    print(f"‚úÖ Dataset CSV sauvegard√©: {df_final.shape}")

    # 2. Fichier PyTorch pour l'entra√Ænement
    torch.save(
        {
            "inputs": input_tensor,
            "targets": target_tensor,
            "num_items": int(
                torch.max(torch.cat([input_tensor.flatten(), target_tensor.flatten()]))
            ),
            "sequence_length": input_tensor.shape[1],
            "selection_method": "smart_no_sklearn",
            "selected_features": selection_stats["final_features"],
            "selection_stats": selection_stats,
        },
        "t4rec_training_data_smart.pt",
    )
    print("‚úÖ Fichier PyTorch sauvegard√©: t4rec_training_data_smart.pt")

    # 3. Analyse de s√©lection
    analysis_text = f"""
# ANALYSE S√âLECTION INTELLIGENTE T4REC (NO-SKLEARN)
================================================

## Pipeline de s√©lection:
1. Analyse variance: {selection_stats.get("step1_variance", 0)} colonnes
2. Suppression corr√©lation: {selection_stats.get("step2_correlation", 0)} colonnes  
3. Optimisation temporelle: {selection_stats.get("step4_temporal", 0)} colonnes
4. S√©lection finale: {selection_stats.get("final_features_count", 0)} colonnes

## M√©thode utilis√©e:
‚úÖ Corr√©lation avec target synth√©tique (au lieu de Random Forest)
‚úÖ Compatible environnements sans sklearn
‚úÖ Performance √©quivalente pour s√©lection de features

## Colonnes finales s√©lectionn√©es:
{chr(10).join(f"- {col}" for col in selection_stats.get("final_features", []))}

## Format T4Rec g√©n√©r√©:
- Input shape: {input_tensor.shape}
- Target shape: {target_tensor.shape}
- Plage ID items: 1 - {torch.max(torch.cat([input_tensor.flatten(), target_tensor.flatten()]))}
- M√©thode: S√©lection intelligente sans sklearn
"""

    with open("column_selection_analysis_no_sklearn.txt", "w", encoding="utf-8") as f:
        f.write(analysis_text)
    print("‚úÖ Analyse sauvegard√©e: column_selection_analysis_no_sklearn.txt")


def main():
    """Pipeline principal - S√âLECTION INTELLIGENTE SANS SKLEARN"""
    print("üöÄ D√âMARRAGE RECIPE S√âLECTION INTELLIGENTE (NO-SKLEARN)")
    print("=" * 60)

    try:
        # 1. Charger et convertir
        df = load_input_dataset()
        df_converted = detect_and_convert_arrays(df)

        # 2. S√©lection intelligente SANS sklearn
        selector = SmartColumnSelectorNoSklearn(df_converted)
        selected_features, selection_stats = selector.smart_selection_pipeline(
            max_features=30,  # Configurable
            variance_threshold=0.01,
            correlation_threshold=0.95,
        )

        # 3. Cr√©er les s√©quences avec features s√©lectionn√©es
        sequences, stats = create_sequences_smart_selection(
            df_converted,
            selected_features,
            seq_length=25,  # S√©quence adapt√©e
        )

        # 4. Format T4Rec
        input_tensor, target_tensor = create_t4rec_format(sequences)

        # 5. Pr√©parer le dataset final avec m√©tadonn√©es
        metadata = {
            "num_sequences": len(sequences),
            "num_unique_items": int(
                torch.max(torch.cat([input_tensor.flatten(), target_tensor.flatten()]))
            ),
            "sequence_length": input_tensor.shape[1],
        }

        # Cr√©er les colonnes du dataset final
        df_final_data = []
        for i in range(len(input_tensor)):
            row = {
                "sequence_id": i,
                "num_sequences": metadata["num_sequences"],
                "num_unique_items": metadata["num_unique_items"],
            }

            # Ajouter les inputs
            for j in range(input_tensor.shape[1]):
                row[f"input_{j + 1}"] = int(input_tensor[i][j])

            # Ajouter les targets
            for j in range(target_tensor.shape[1]):
                row[f"target_{j + 1}"] = int(target_tensor[i][j])

            df_final_data.append(row)

        df_final = pd.DataFrame(df_final_data)

        # 6. Sauvegarder tout
        save_outputs(df_final, input_tensor, target_tensor, selection_stats)

        print("\nüéâ RECIPE TERMIN√âE AVEC SUCC√àS!")
        print(
            f"üìä S√©lection: {len(df_converted.columns)} ‚Üí {len(selected_features)} colonnes"
        )
        print(f"üìä S√©quences: {sequences.shape}")
        print(
            f"üìä Format T4Rec: inputs{input_tensor.shape}, targets{target_tensor.shape}"
        )

    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
