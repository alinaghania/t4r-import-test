import dataiku
import numpy as np
import pandas as pd
import json
import torch
import os
from typing import List, Tuple, Dict, Any


def load_input_dataset() -> pd.DataFrame:
    """
    Charge le dataset d'input depuis Dataiku
    """
    try:
        dataset = dataiku.Dataset("t4_rec_df")
        df = dataset.get_dataframe()
        print(f"‚úÖ Dataset input 't4_rec_df' charg√©: {df.shape}")
        return df
    except Exception as e:
        print(f"‚ùå Erreur chargement input: {e}")
        raise


def detect_and_convert_arrays(df: pd.DataFrame) -> pd.DataFrame:
    """
    D√©tecte et convertit toutes les colonnes qui contiennent des arrays JSON
    """
    print(f"üîç Analyse de {len(df.columns)} colonnes...")

    converted_df = df.copy()
    conversion_stats = {"array_columns": 0, "converted_successfully": 0, "errors": 0}

    for col in df.columns:
        # √âchantillon pour d√©tecter le format
        sample_values = df[col].dropna().head(10)

        if len(sample_values) == 0:
            continue

        first_val = str(sample_values.iloc[0])

        # D√©tecter si c'est un array JSON
        if first_val.startswith("[") and first_val.endswith("]"):
            conversion_stats["array_columns"] += 1
            print(f"  üìä Conversion colonne '{col}': {first_val[:50]}...")

            try:
                # Convertir chaque valeur JSON en float
                def convert_json_array(val):
                    if pd.isna(val):
                        return 0.0
                    try:
                        arr = json.loads(str(val))
                        if isinstance(arr, list) and len(arr) > 0:
                            return float(arr[0])  # Prendre le premier √©l√©ment
                        return 0.0
                    except:
                        return 0.0

                converted_df[col] = df[col].apply(convert_json_array)
                conversion_stats["converted_successfully"] += 1

            except Exception as e:
                print(f"    ‚ùå Erreur conversion {col}: {e}")
                conversion_stats["errors"] += 1
                # Garder la colonne originale en cas d'erreur

    print(f"üìà R√©sultats conversion:")
    print(f"  - Colonnes array d√©tect√©es: {conversion_stats['array_columns']}")
    print(f"  - Converties avec succ√®s: {conversion_stats['converted_successfully']}")
    print(f"  - Erreurs: {conversion_stats['errors']}")

    return converted_df


def create_simple_sequences(
    df: pd.DataFrame, seq_length: int = 10
) -> Tuple[np.ndarray, Dict]:
    """
    Cr√©e des s√©quences simples en prenant les valeurs num√©riques de chaque ligne
    et en les regroupant pour former des s√©quences
    """
    print(f"üîÑ Cr√©ation de s√©quences de longueur {seq_length}...")

    # S√©lectionner seulement les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print(f"  üìä {len(numeric_cols)} colonnes num√©riques trouv√©es")

    if len(numeric_cols) == 0:
        raise ValueError("Aucune colonne num√©rique trouv√©e!")

    # Prendre un sous-ensemble de colonnes si n√©cessaire
    if len(numeric_cols) > seq_length * 2:  # Garder assez pour input/target
        # Prendre les premi√®res colonnes (ou on pourrait randomiser)
        selected_cols = numeric_cols[: seq_length * 2]
        print(f"  ‚úÇÔ∏è S√©lection de {len(selected_cols)} colonnes sur {len(numeric_cols)}")
    else:
        selected_cols = numeric_cols

    sequences = []

    for idx, row in df.iterrows():
        # R√©cup√©rer les valeurs num√©riques de la ligne
        values = [row[col] for col in selected_cols]

        # Filtrer les valeurs NaN/infinies
        values = [v for v in values if not (pd.isna(v) or np.isinf(v))]

        if len(values) < seq_length:
            # R√©p√©ter les valeurs ou padding si pas assez
            while len(values) < seq_length:
                values.extend(values)  # R√©p√©ter
            values = values[:seq_length]  # Tronquer
        else:
            # Prendre les premi√®res valeurs
            values = values[:seq_length]

        sequences.append(values)

        if idx % 1000 == 0:
            print(f"  üìù Trait√© {idx + 1} lignes...")

    sequences_array = np.array(sequences)
    print(f"‚úÖ S√©quences cr√©√©es: {sequences_array.shape}")

    # Stats sur les s√©quences
    stats = {
        "num_sequences": len(sequences),
        "sequence_length": seq_length,
        "selected_columns": selected_cols,
        "value_range": (float(np.min(sequences_array)), float(np.max(sequences_array))),
        "mean_value": float(np.mean(sequences_array)),
    }

    return sequences_array, stats


def create_t4rec_format(sequences: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Convertit les s√©quences au format T4Rec (input/target pairs)
    """
    print(f"üéØ Conversion au format T4Rec...")

    # Pour T4Rec, on a besoin d'item_ids (entiers positifs)
    # Normaliser et convertir en IDs

    # 1. Normaliser les valeurs entre 1 et 1000 (simule des item_ids)
    sequences_norm = sequences.copy()

    # Normalisation min-max vers [1, 1000]
    seq_min = np.min(sequences_norm)
    seq_max = np.max(sequences_norm)

    if seq_max != seq_min:
        sequences_norm = (sequences_norm - seq_min) / (seq_max - seq_min)
        sequences_norm = sequences_norm * 999 + 1  # [1, 1000]
    else:
        sequences_norm = np.ones_like(sequences_norm)  # Toutes les valeurs = 1

    # 2. Convertir en entiers (item_ids)
    sequences_ids = sequences_norm.astype(int)

    # 3. Cr√©er input/target pairs pour next-item prediction
    # Input: [1, 2, 3, ..., n-1]
    # Target: [2, 3, 4, ..., n]

    inputs = sequences_ids[:, :-1]  # Tout sauf le dernier
    targets = sequences_ids[:, 1:]  # Tout sauf le premier

    # 4. Convertir en tenseurs PyTorch
    input_tensor = torch.LongTensor(inputs)
    target_tensor = torch.LongTensor(targets)

    print(f"  üìä Input shape: {input_tensor.shape}")
    print(f"  üìä Target shape: {target_tensor.shape}")
    print(
        f"  üìä Item ID range: {int(np.min(sequences_ids))} - {int(np.max(sequences_ids))}"
    )

    return input_tensor, target_tensor


def create_output_dataframe(
    inputs: torch.Tensor, targets: torch.Tensor, original_df: pd.DataFrame, stats: Dict
) -> pd.DataFrame:
    """
    Cr√©e le DataFrame de sortie pour Dataiku (VERSION CSV)
    """
    print(f"üìã Cr√©ation du DataFrame CSV pour Dataiku...")

    # Convertir les tenseurs en numpy pour le DataFrame
    inputs_np = inputs.numpy()
    targets_np = targets.numpy()

    # Cr√©er le DataFrame de sortie
    output_data = []

    for i in range(len(inputs_np)):
        # Cr√©er une ligne par s√©quence
        row_data = {
            "sequence_id": i,
            "sequence_length": len(inputs_np[i]),
            "num_unique_items": stats.get("num_unique_items", 1000),
        }

        # Ajouter les inputs individuels comme colonnes
        for j, val in enumerate(inputs_np[i]):
            row_data[f"input_{j + 1}"] = int(val)

        # Ajouter les targets individuels comme colonnes
        for j, val in enumerate(targets_np[i]):
            row_data[f"target_{j + 1}"] = int(val)

        output_data.append(row_data)

    output_df = pd.DataFrame(output_data)

    print(f"‚úÖ DataFrame CSV cr√©√©: {output_df.shape}")
    print(
        f"  üìä Colonnes: {list(output_df.columns[:5])}... (+ {len(output_df.columns) - 5} autres)"
    )

    return output_df


def save_pytorch_tensors(
    inputs: torch.Tensor, targets: torch.Tensor, stats: Dict
) -> str:
    """
    Sauvegarde les tenseurs PyTorch dans un fichier .pt pr√™t pour l'entra√Ænement
    """
    print(f"üíæ Sauvegarde des tenseurs PyTorch...")

    # Cr√©er le dictionnaire avec toutes les donn√©es n√©cessaires pour l'entra√Ænement
    data_dict = {
        "inputs": inputs,
        "targets": targets,
        "metadata": {
            "num_sequences": inputs.shape[0],
            "sequence_length": inputs.shape[1],
            "num_unique_items": int(
                torch.max(torch.cat([inputs.flatten(), targets.flatten()])).item()
            ),
            "creation_timestamp": pd.Timestamp.now().isoformat(),
            "selected_columns": stats.get("selected_columns", []),
            "value_range": stats.get("value_range", (0, 1000)),
            "mean_value": stats.get("mean_value", 0.0),
        },
    }

    # Nom du fichier simple
    filename = "t4rec_training_data.pt"

    # Sauvegarde
    torch.save(data_dict, filename)
    print(f"‚úÖ Tenseurs PyTorch sauvegard√©s: {filename}")
    print(f"  üì¶ Contenu: inputs {inputs.shape}, targets {targets.shape}")

    return filename


def save_to_output_dataset(df: pd.DataFrame) -> None:
    """
    Sauvegarde le DataFrame vers le dataset de sortie Dataiku
    """
    try:
        output_dataset = dataiku.Dataset("t4_rec_df_clean")
        output_dataset.write_with_schema(df)
        print(f"‚úÖ Dataset CSV 't4_rec_df_clean' cr√©√© avec {len(df)} lignes")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde output: {e}")
        raise


def create_usage_guide(pytorch_file: str, stats: Dict) -> None:
    """
    Cr√©e un guide d'utilisation pour charger les donn√©es dans le mod√®le
    """
    guide = f"""
# GUIDE D'UTILISATION T4REC
========================

## Fichiers g√©n√©r√©s:
1. Dataset Dataiku: 't4_rec_df_clean' (visualisation/debug)
2. Fichier PyTorch: '{pytorch_file}' (entra√Ænement direct)

## Charger les donn√©es pour l'entra√Ænement:
```python
import torch

# Charger les donn√©es T4Rec
data = torch.load('{pytorch_file}')
inputs = data['inputs']      # torch.Size([{stats["num_sequences"]}, {stats["sequence_length"] - 1}])
targets = data['targets']    # torch.Size([{stats["num_sequences"]}, {stats["sequence_length"] - 1}])
metadata = data['metadata']

# Utiliser avec ton mod√®le T4Rec
from src.transformer_model import TransformerRecommendationModel
from src.model_trainer import ModelTrainer

model = TransformerRecommendationModel(
    num_items=metadata['num_unique_items'],  # {stats.get("num_unique_items", 1000)}
    embedding_dim=64,
    seq_length={stats["sequence_length"]}
)

trainer = ModelTrainer(model)
trainer.train(inputs, targets, num_epochs=5)
```

## Donn√©es disponibles:
- S√©quences d'origine: {stats["num_sequences"]}
- Longueur de s√©quence: {stats["sequence_length"]}
- Range item_ids: 1 - {stats.get("num_unique_items", 1000)}
- Colonnes utilis√©es: {len(stats.get("selected_columns", []))}
"""

    with open("t4rec_usage_guide.txt", "w", encoding="utf-8") as f:
        f.write(guide)

    print(f"üìö Guide d'utilisation cr√©√©: t4rec_usage_guide.txt")


def main():
    """
    Pipeline principal du recipe Dataiku
    """
    print("üöÄ D√âMARRAGE RECIPE DATAIKU T4REC")
    print("=" * 50)

    try:
        # 1. Charger les donn√©es d'input
        df = load_input_dataset()
        print(f"üìä Dataset initial: {df.shape}")

        # 2. Convertir les arrays JSON
        df_converted = detect_and_convert_arrays(df)

        # 3. Cr√©er des s√©quences simples
        sequences, stats = create_simple_sequences(df_converted, seq_length=10)
        print(f"üìà Stats s√©quences: {stats}")

        # 4. Convertir au format T4Rec
        inputs, targets = create_t4rec_format(sequences)

        # Ajouter les stats pour le DataFrame de sortie
        stats["num_unique_items"] = int(
            torch.max(torch.cat([inputs.flatten(), targets.flatten()])).item()
        )

        # 5. DOUBLE SAUVEGARDE

        # 5a. Cr√©er le DataFrame CSV pour Dataiku
        output_df = create_output_dataframe(inputs, targets, df, stats)
        save_to_output_dataset(output_df)

        # 5b. Sauvegarder les tenseurs PyTorch pour l'entra√Ænement
        pytorch_file = save_pytorch_tensors(inputs, targets, stats)

        # 6. Cr√©er le guide d'utilisation
        create_usage_guide(pytorch_file, stats)

        print("\nüéâ RECIPE TERMIN√â AVEC SUCC√àS!")
        print("=" * 50)
        print("üìã DOUBLE SORTIE CR√â√âE:")
        print(f"  1. Dataset CSV Dataiku: 't4_rec_df_clean' ({len(output_df)} lignes)")
        print(f"  2. Tenseurs PyTorch: '{pytorch_file}'")
        print("  3. Guide d'usage: 't4rec_usage_guide.txt'")
        print("\nüéØ PR√äT POUR L'ENTRA√éNEMENT!")

        # Test rapide de compatibilit√©
        print("\nüî¨ R√âSUM√â TECHNIQUE:")
        print(f"  - S√©quences d'origine: {len(df)} ‚Üí {len(output_df)} s√©quences T4Rec")
        print(f"  - Format inputs: {inputs.dtype} {inputs.shape}")
        print(f"  - Format targets: {targets.dtype} {targets.shape}")
        print(f"  - Item ID range: 1 - {stats['num_unique_items']}")
        print("  ‚úÖ Compatible avec ton mod√®le T4Rec!")

    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
