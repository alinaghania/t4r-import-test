"""
SYST√àME DE RECOMMANDATION BANCAIRE AVEC TRANSFORMERS
====================================================

Ce script impl√©mente un syst√®me de recommandation pour produits bancaires utilisant
l'architecture Transformer. Le mod√®le pr√©dit le prochain produit qu'un client va
consulter/acheter bas√© sur son historique de navigation.

Architecture:
- Embeddings pour repr√©senter les produits bancaires (64 dimensions)
- Transformer Encoder (2 couches, 4 t√™tes d'attention) pour capturer les s√©quences
- Couche de sortie pour pr√©diction du prochain item (next item prediction)

"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn


def create_sample_banking_data():
    """
    G√©n√®re des donn√©es bancaires synth√©tiques pour l'entra√Ænement.

    Structure des donn√©es:
    - customer_id: Identifiant unique du client (1-1000)
    - item_id: Produit bancaire consult√©/achet√© (1-50)
    - timestamp: Horodatage de l'interaction
    - session_id: Identifiant de session de navigation (0-4999)
    - amount: Montant de la transaction (10-1000‚Ç¨)

    Returns:
        pd.DataFrame: DataFrame avec les interactions bancaires
    """
    np.random.seed(42)  # Pour la reproductibilit√©
    n_customers = 1000  # Nombre de clients
    n_products = 50  # Nombre de produits bancaires diff√©rents
    n_sessions = 5000  # Nombre de sessions de navigation
    data = []

    print("G√©n√©ration des donn√©es bancaires synth√©tiques...")

    for session_id in range(n_sessions):
        # Chaque session appartient √† un client al√©atoire
        customer_id = np.random.randint(1, n_customers + 1)

        # Longueur variable de session (2-9 interactions)
        session_length = np.random.randint(2, 10)

        # Timestamp de base pour la session
        base_timestamp = pd.Timestamp("2024-01-01") + pd.Timedelta(
            days=np.random.randint(0, 365)
        )

        # G√©n√©rer les interactions de la session
        for i in range(session_length):
            data.append(
                {
                    "customer_id": customer_id,
                    "item_id": np.random.randint(1, n_products + 1),
                    "timestamp": base_timestamp + pd.Timedelta(minutes=i * 5),
                    "session_id": session_id,
                    "amount": np.random.uniform(10, 1000),
                }
            )

    # Cr√©er le DataFrame et trier par session et timestamp
    df = pd.DataFrame(data)
    df = df.sort_values(["session_id", "timestamp"])

    # S'assurer que les types sont corrects
    df["session_id"] = df["session_id"].astype(np.int64)
    df["customer_id"] = df["customer_id"].astype(np.int64)
    df["item_id"] = df["item_id"].astype(np.int64)

    return df


class SimpleBankingRecommenderModel(nn.Module):
    """
    Mod√®le de recommandation bancaire bas√© sur l'architecture Transformer.

    Le mod√®le utilise:
    1. Des embeddings pour repr√©senter les produits bancaires
    2. Un Transformer Encoder pour capturer les d√©pendances s√©quentielles
    3. Une couche de sortie pour pr√©dire le prochain produit

    Args:
        num_items (int): Nombre total de produits bancaires
        embedding_dim (int): Dimension des embeddings (d√©faut: 64)
        seq_length (int): Longueur maximale des s√©quences (d√©faut: 10)
    """

    def __init__(self, num_items, embedding_dim=64, seq_length=10):
        super().__init__()
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        self.seq_length = seq_length

        # Couche d'embedding pour les produits bancaires
        # padding_idx=0 pour que le token de padding ait un embedding fixe √† z√©ro
        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)

        # Architecture Transformer pour capturer les s√©quences
        # - 2 couches d'encodeur pour capturer les d√©pendances complexes
        # - 4 t√™tes d'attention pour diff√©rents aspects des relations
        # - Dimension feedforward 2x plus grande (128) pour plus de capacit√©
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embedding_dim,  # Dimension du mod√®le
                nhead=4,  # Nombre de t√™tes d'attention
                dim_feedforward=128,  # Dimension du feedforward
                dropout=0.1,  # Dropout pour la r√©gularisation
                batch_first=True,  # Format [batch, seq, features]
            ),
            num_layers=2,  # Nombre de couches d'encodeur
        )

        # Couche de sortie pour pr√©dire le prochain produit
        # +1 pour inclure le token de padding dans les pr√©dictions
        self.output_layer = nn.Linear(embedding_dim, num_items + 1)

    def forward(self, item_sequence):
        """
        Forward pass du mod√®le.

        Args:
            item_sequence (torch.Tensor): S√©quence d'items [batch_size, seq_length]

        Returns:
            torch.Tensor: Logits pour la pr√©diction du prochain item [batch_size, num_items+1]
        """
        # √âtape 1: Convertir les IDs en embeddings
        embeddings = self.item_embedding(item_sequence)  # [batch, seq_len, embed_dim]

        # √âtape 2: Passer par le Transformer pour capturer les d√©pendances
        sequence_output = self.transformer(embeddings)  # [batch, seq_len, embed_dim]

        # √âtape 3: Utiliser la repr√©sentation du dernier token pour la pr√©diction
        # Le dernier token contient l'information de toute la s√©quence
        last_output = sequence_output[:, -1, :]  # [batch, embed_dim]

        # √âtape 4: Pr√©dire le prochain item
        logits = self.output_layer(last_output)  # [batch, num_items+1]

        return logits


def create_target_sequence(item_sequence):
    """
    Cr√©e les s√©quences d'entr√©e et de cible pour l'entra√Ænement.

    Pour l'entra√Ænement "next item prediction":
    - Input: tous les items sauf le dernier  [item1, item2, ..., itemN-1]
    - Target: tous les items sauf le premier [item2, item3, ..., itemN]

    Args:
        item_sequence (torch.Tensor): S√©quences compl√®tes [batch, seq_len]

    Returns:
        tuple: (inputs, targets) pour l'entra√Ænement
    """
    inputs = item_sequence[:, :-1]  # Tous sauf le dernier
    targets = item_sequence[:, 1:]  # Tous sauf le premier

    return inputs, targets


def main():
    """Fonction principale du pipeline d'entra√Ænement."""

    print("=" * 60)
    print("üè¶ SYST√àME DE RECOMMANDATION BANCAIRE AVEC TRANSFORMERS üè¶")
    print("=" * 60)

    # ==========================================
    # 1. G√âN√âRATION DES DONN√âES
    # ==========================================
    print("\n=== 1. G√âN√âRATION DES DONN√âES ===")
    df = create_sample_banking_data()
    print(f"‚úÖ Donn√©es g√©n√©r√©es: {df.shape[0]:,} transactions")
    print(f"   ‚Ä¢ Sessions: {df['session_id'].nunique():,}")
    print(f"   ‚Ä¢ Clients: {df['customer_id'].nunique():,}")
    print(f"   ‚Ä¢ Produits: {df['item_id'].nunique()}")
    print("\nAper√ßu des donn√©es:")
    print(df.head())

    # Sauvegarder les donn√©es brutes
    os.makedirs("processed_data/seq_data", exist_ok=True)
    raw_path = "processed_data/seq_data/banking_data_raw.parquet"
    df.to_parquet(raw_path, index=False)
    print(f"üíæ Donn√©es sauvegard√©es: {raw_path}")

    # ==========================================
    # 2. PR√âPARATION DES S√âQUENCES
    # ==========================================
    print("\n=== 2. PR√âPARATION DES S√âQUENCES ===")
    max_seq_length = 10  # Longueur maximale des s√©quences
    sequences = []

    print("Conversion des sessions en s√©quences...")
    for session_id, session_data in df.groupby("session_id"):
        session_data = session_data.sort_values("timestamp")
        item_seq = session_data["item_id"].tolist()

        # Padding et troncature pour uniformiser les longueurs
        if len(item_seq) > max_seq_length:
            # Garder les derniers items (plus r√©cents)
            item_seq = item_seq[-max_seq_length:]
        else:
            # Ajouter du padding (0) au d√©but
            item_seq = item_seq + [0] * (max_seq_length - len(item_seq))

        sequences.append(item_seq)

    sequences = np.array(sequences)
    print(f"‚úÖ S√©quences cr√©√©es: {sequences.shape}")
    print(f"   ‚Ä¢ Forme: {sequences.shape[0]:,} sessions √ó {sequences.shape[1]} items")
    print(f"   ‚Ä¢ Exemple: {sequences[0]}")

    # ==========================================
    # 3. CR√âATION DU MOD√àLE
    # ==========================================
    print("\n=== 3. CR√âATION DU MOD√àLE ===")
    num_items = 50  # Nombre maximum d'items dans nos donn√©es
    model = SimpleBankingRecommenderModel(
        num_items=num_items, embedding_dim=64, seq_length=max_seq_length
    )

    # Configuration du device (GPU si disponible, sinon CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è  Device utilis√©: {device}")
    model = model.to(device)

    # Afficher l'architecture du mod√®le
    print("\nüìä Architecture du mod√®le:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ‚Ä¢ Param√®tres totaux: {total_params:,}")
    print(f"   ‚Ä¢ Param√®tres entra√Ænables: {trainable_params:,}")
    print(f"   ‚Ä¢ Embedding dimension: {64}")
    print(f"   ‚Ä¢ Couches Transformer: 2")
    print(f"   ‚Ä¢ T√™tes d'attention: 4")

    # ==========================================
    # 4. PR√âPARATION DES DONN√âES D'ENTRA√éNEMENT
    # ==========================================
    print("\n=== 4. PR√âPARATION DES DONN√âES D'ENTRA√éNEMENT ===")

    # Convertir en tenseurs PyTorch
    sequences_tensor = torch.LongTensor(sequences).to(device)

    # Cr√©er les paires input/target pour next item prediction
    inputs, targets = create_target_sequence(sequences_tensor)

    print(f"‚úÖ Donn√©es d'entra√Ænement pr√©par√©es:")
    print(f"   ‚Ä¢ Inputs shape: {inputs.shape}")
    print(f"   ‚Ä¢ Targets shape: {targets.shape}")
    print(f"   ‚Ä¢ Exemple input: {inputs[0].cpu().numpy()}")
    print(f"   ‚Ä¢ Exemple target: {targets[0].cpu().numpy()}")

    # ==========================================
    # 5. ENTRA√éNEMENT DU MOD√àLE
    # ==========================================
    print("\n=== 5. ENTRA√éNEMENT DU MOD√àLE ===")

    # Configuration de l'entra√Ænement
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignorer le padding
    batch_size = 64
    num_epochs = 5

    print(f"‚öôÔ∏è  Configuration:")
    print(f"   ‚Ä¢ Optimiseur: Adam (lr=1e-3)")
    print(f"   ‚Ä¢ Loss: CrossEntropyLoss")
    print(f"   ‚Ä¢ Batch size: {batch_size}")
    print(f"   ‚Ä¢ √âpoques: {num_epochs}")

    model.train()
    print(f"\nüöÄ D√©but de l'entra√Ænement...")

    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0

        # Entra√Ænement par mini-batch
        for i in range(0, len(inputs), batch_size):
            batch_inputs = inputs[i : i + batch_size]
            batch_targets = targets[i : i + batch_size]

            if len(batch_inputs) == 0:
                continue

            optimizer.zero_grad()

            # Training avec pr√©diction s√©quentielle
            batch_size_actual = batch_inputs.size(0)
            seq_len = batch_inputs.size(1)
            all_losses = []

            # Pr√©dire √† chaque position de la s√©quence
            for pos in range(seq_len):
                if pos == 0:
                    continue  # Pas de pr√©diction sans contexte

                # Utiliser la s√©quence jusqu'√† la position pos
                input_seq = batch_inputs[:, : pos + 1]

                # Padding si n√©cessaire
                if input_seq.size(1) < max_seq_length:
                    padding = torch.zeros(
                        batch_size_actual,
                        max_seq_length - input_seq.size(1),
                        device=device,
                        dtype=torch.long,
                    )
                    input_seq = torch.cat([padding, input_seq], dim=1)

                # Forward pass
                logits = model(input_seq)
                target_items = batch_targets[:, pos - 1]

                # Loss seulement pour les tokens non-padding
                mask = target_items != 0
                if mask.sum() > 0:
                    loss = criterion(logits[mask], target_items[mask])
                    all_losses.append(loss)

            # Backpropagation
            if all_losses:
                total_batch_loss = torch.stack(all_losses).mean()
                total_batch_loss.backward()
                optimizer.step()

                total_loss += total_batch_loss.item()
                num_batches += 1

        # Afficher le progr√®s
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        print(f"   √âpoque {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f}")

    print("‚úÖ Entra√Ænement termin√© !")

    # ==========================================
    # 6. TEST ET √âVALUATION
    # ==========================================
    print("\n=== 6. TEST ET √âVALUATION ===")
    model.eval()

    with torch.no_grad():
        test_sequences = sequences_tensor[:5]  # Tester sur 5 s√©quences

        print("üîç Tests de pr√©diction:")
        for i, seq in enumerate(test_sequences):
            # Utiliser la s√©quence sans le dernier item pour pr√©dire
            input_seq = seq[:-1].unsqueeze(0)
            actual_next = seq[-1].item()

            # Padding si n√©cessaire
            if input_seq.size(1) < max_seq_length:
                padding = torch.zeros(
                    1,
                    max_seq_length - input_seq.size(1),
                    device=device,
                    dtype=torch.long,
                )
                input_seq = torch.cat([padding, input_seq], dim=1)

            # Pr√©diction
            logits = model(input_seq)
            predicted_probs = torch.softmax(logits, dim=1)
            top_predictions = torch.topk(predicted_probs, k=5, dim=1)

            print(f"\n   S√©quence {i + 1}:")
            print(f"     Input: {input_seq.squeeze().cpu().numpy()}")
            print(f"     Item r√©el: {actual_next}")
            print(f"     Top 5: {top_predictions.indices.squeeze().cpu().numpy()}")
            print(f"     Proba: {top_predictions.values.squeeze().cpu().numpy()}")

    # ==========================================
    # 7. SAUVEGARDE DU MOD√àLE
    # ==========================================
    print("\n=== 7. SAUVEGARDE DU MOD√àLE ===")
    model_path = "banking_transformer_rec_model.pt"
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "model_config": {
                "num_items": num_items,
                "embedding_dim": 64,
                "seq_length": max_seq_length,
            },
            "training_info": {
                "num_epochs": num_epochs,
                "final_loss": avg_loss,
                "num_sequences": len(sequences),
                "num_transactions": len(df),
            },
        },
        model_path,
    )

    print(f"üíæ Mod√®le sauvegard√©: {model_path}")

    # ==========================================
    # R√âSUM√â FINAL
    # ==========================================
    print("\n" + "=" * 60)
    print("üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS ! üéâ")
    print("=" * 60)
    print(f"‚úÖ Donn√©es: {df.shape[0]:,} transactions, {len(sequences):,} sessions")
    print(f"‚úÖ Mod√®le: Transformer {num_items} items, {64}D embeddings")
    print(f"‚úÖ Entra√Ænement: {max_seq_length} seq_length, {num_epochs} √©poques")
    print(f"‚úÖ Performance: Loss finale {avg_loss:.4f}")
    print("‚úÖ Pipeline complet de recommandation bancaire op√©rationnel !")
    print("\nüöÄ Le mod√®le est pr√™t pour la production !")


if __name__ == "__main__":
    main()
