"""
MODULE: PREPROCESSEUR DE DONN√âES
==============================

Ce module transforme les donn√©es de transactions brutes en s√©quences
pr√™tes pour l'entra√Ænement du mod√®le Transformer.

Auteur: Assistant AI & Alina Ghani
Date: Juillet 2025
"""

import os
import pandas as pd
import numpy as np
import torch
import logging
from typing import Tuple, List

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SequenceDataPreprocessor:
    """
    Preprocesseur pour transformer les donn√©es de transactions en s√©quences.

    Cette classe g√®re:
    - La conversion des sessions en s√©quences
    - Le padding/troncature des s√©quences
    - La cr√©ation des paires input/target pour l'entra√Ænement
    """

    def __init__(self, max_seq_length=10):
        """
        Initialise le preprocesseur.

        Args:
            max_seq_length (int): Longueur maximale des s√©quences
        """
        self.max_seq_length = max_seq_length

        logger.info(f"Preprocesseur initialis√©:")
        logger.info(f"  ‚Ä¢ Longueur max s√©quences: {max_seq_length}")

    def load_data(self, data_path):
        """
        Charge les donn√©es de transactions depuis un fichier.

        Args:
            data_path (str): Chemin vers le fichier de donn√©es

        Returns:
            pd.DataFrame: Donn√©es charg√©es
        """
        logger.info(f"üìÇ Chargement des donn√©es depuis {data_path}...")

        if data_path.endswith(".parquet"):
            df = pd.read_parquet(data_path)
        elif data_path.endswith(".csv"):
            df = pd.read_csv(data_path)
        else:
            raise ValueError(f"Format de fichier non support√©: {data_path}")

        logger.info(f"‚úÖ Donn√©es charg√©es: {df.shape[0]:,} lignes")

        return df

    def create_sequences(self, df):
        """
        Convertit les sessions en s√©quences d'items.

        Args:
            df (pd.DataFrame): DataFrame avec colonnes session_id, item_id, timestamp

        Returns:
            np.ndarray: Array de s√©quences [n_sessions, max_seq_length]
        """
        logger.info("üîÑ Conversion des sessions en s√©quences...")

        sequences = []
        session_stats = {"lengths": [], "total_sessions": 0}

        # Grouper par session et cr√©er les s√©quences
        for session_id, session_data in df.groupby("session_id"):
            session_stats["total_sessions"] += 1

            # Trier par timestamp pour respecter l'ordre chronologique
            session_data = session_data.sort_values("timestamp")
            item_seq = session_data["item_id"].tolist()

            # Stocker la longueur originale
            session_stats["lengths"].append(len(item_seq))

            # Padding et troncature pour uniformiser les longueurs
            if len(item_seq) > self.max_seq_length:
                # Garder les derniers items (plus r√©cents)
                item_seq = item_seq[-self.max_seq_length :]
            else:
                # Ajouter du padding (0) √† la fin
                item_seq = item_seq + [0] * (self.max_seq_length - len(item_seq))

            sequences.append(item_seq)

            # Progression
            if len(sequences) % 1000 == 0:
                logger.info(f"  S√©quences trait√©es: {len(sequences)}")

        sequences = np.array(sequences)

        # Statistiques
        lengths = np.array(session_stats["lengths"])
        logger.info("‚úÖ S√©quences cr√©√©es !")
        logger.info(f"  ‚Ä¢ Nombre de s√©quences: {sequences.shape[0]:,}")
        logger.info(f"  ‚Ä¢ Forme finale: {sequences.shape}")
        logger.info(f"  ‚Ä¢ Longueur moyenne originale: {lengths.mean():.1f}")
        logger.info(f"  ‚Ä¢ Longueur m√©diane originale: {np.median(lengths):.1f}")
        logger.info(f"  ‚Ä¢ Sessions tronqu√©es: {(lengths > self.max_seq_length).sum()}")
        logger.info(f"  ‚Ä¢ Sessions padd√©es: {(lengths < self.max_seq_length).sum()}")

        return sequences

    def create_training_pairs(self, sequences):
        """
        Cr√©e les paires input/target pour l'entra√Ænement next-item prediction.

        Args:
            sequences (np.ndarray): S√©quences d'items [n_sessions, seq_length]

        Returns:
            tuple: (inputs, targets) pour l'entra√Ænement
        """
        logger.info("üéØ Cr√©ation des paires input/target...")

        # Convertir en tenseurs PyTorch
        sequences_tensor = torch.LongTensor(sequences)

        # Pour next-item prediction:
        # - Input: tous les items sauf le dernier
        # - Target: tous les items sauf le premier (d√©calage d'une position)
        inputs = sequences_tensor[:, :-1]  # [batch, seq_len-1]
        targets = sequences_tensor[:, 1:]  # [batch, seq_len-1]

        logger.info("‚úÖ Paires cr√©√©es !")
        logger.info(f"  ‚Ä¢ Inputs shape: {inputs.shape}")
        logger.info(f"  ‚Ä¢ Targets shape: {targets.shape}")

        # Exemples
        logger.info("üìã Exemples:")
        for i in range(min(3, len(inputs))):
            logger.info(f"  S√©quence {i + 1}:")
            logger.info(f"    Input:  {inputs[i].numpy()}")
            logger.info(f"    Target: {targets[i].numpy()}")

        return inputs, targets

    def save_sequences(self, sequences, output_path="data/processed_sequences.pt"):
        """
        Sauvegarde les s√©quences preprocess√©es.

        Args:
            sequences (np.ndarray): S√©quences √† sauvegarder
            output_path (str): Chemin de sauvegarde
        """
        # Cr√©er le r√©pertoire si n√©cessaire
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Sauvegarder en format PyTorch
        torch.save(
            {
                "sequences": sequences,
                "max_seq_length": self.max_seq_length,
                "shape": sequences.shape,
                "preprocessing_info": {
                    "num_sequences": len(sequences),
                    "seq_length": self.max_seq_length,
                    "unique_items": len(np.unique(sequences[sequences > 0])),
                },
            },
            output_path,
        )

        logger.info(f"üíæ S√©quences sauvegard√©es: {output_path}")

        return output_path

    def get_data_stats(self, sequences):
        """
        Calcule les statistiques des donn√©es preprocess√©es.

        Args:
            sequences (np.ndarray): S√©quences preprocess√©es

        Returns:
            dict: Statistiques des donn√©es
        """
        # Items uniques (sans padding)
        non_zero_items = sequences[sequences > 0]
        unique_items = np.unique(non_zero_items)

        # Statistiques par s√©quence
        seq_lengths = []
        for seq in sequences:
            # Compter les items non-padding
            length = np.sum(seq > 0)
            seq_lengths.append(length)

        stats = {
            "num_sequences": len(sequences),
            "max_seq_length": self.max_seq_length,
            "unique_items": len(unique_items),
            "item_range": (unique_items.min(), unique_items.max()),
            "avg_seq_length": np.mean(seq_lengths),
            "median_seq_length": np.median(seq_lengths),
            "total_interactions": len(non_zero_items),
            "padding_ratio": np.sum(sequences == 0) / sequences.size,
        }

        return stats

    def full_preprocessing_pipeline(
        self,
        data_path,
        output_sequences_path="data/processed_sequences.pt",
        output_training_path="data/training_data.pt",
    ):
        """
        Pipeline complet de preprocessing.

        Args:
            data_path (str): Chemin vers les donn√©es brutes
            output_sequences_path (str): Chemin de sauvegarde des s√©quences
            output_training_path (str): Chemin de sauvegarde des donn√©es d'entra√Ænement

        Returns:
            dict: R√©sultats du preprocessing
        """
        # 1. Charger les donn√©es
        df = self.load_data(data_path)

        # 2. Cr√©er les s√©quences
        sequences = self.create_sequences(df)

        # 3. Cr√©er les paires d'entra√Ænement
        inputs, targets = self.create_training_pairs(sequences)

        # 4. Sauvegarder les s√©quences
        self.save_sequences(sequences, output_sequences_path)

        # 5. Sauvegarder les donn√©es d'entra√Ænement
        os.makedirs(os.path.dirname(output_training_path), exist_ok=True)
        torch.save(
            {
                "inputs": inputs,
                "targets": targets,
                "sequences": sequences,
                "preprocessing_config": {"max_seq_length": self.max_seq_length},
            },
            output_training_path,
        )

        # 6. Calculer les statistiques
        stats = self.get_data_stats(sequences)

        logger.info(f"üíæ Donn√©es d'entra√Ænement sauvegard√©es: {output_training_path}")
        logger.info("üìä Statistiques finales:")
        for key, value in stats.items():
            logger.info(f"  ‚Ä¢ {key}: {value}")

        return {
            "sequences": sequences,
            "inputs": inputs,
            "targets": targets,
            "stats": stats,
            "paths": {
                "sequences": output_sequences_path,
                "training": output_training_path,
            },
        }


def main():
    """Fonction principale pour test du module."""
    print("=" * 60)
    print("‚öôÔ∏è PREPROCESSING DES DONN√âES")
    print("=" * 60)

    # Cr√©er le preprocesseur
    preprocessor = SequenceDataPreprocessor(max_seq_length=10)

    # Pipeline complet (en supposant que les donn√©es existent)
    try:
        results = preprocessor.full_preprocessing_pipeline(
            data_path="data/raw_transactions.parquet",
            output_sequences_path="data/processed_sequences.pt",
            output_training_path="data/training_data.pt",
        )

        print(f"\n‚úÖ Preprocessing termin√© avec succ√®s!")
        print(f"üìÅ S√©quences: {results['paths']['sequences']}")
        print(f"üìÅ Entra√Ænement: {results['paths']['training']}")

    except FileNotFoundError:
        print("Fichier de donn√©es non trouv√©.")
        print("Ex√©cutez d'abord data_generator.py pour cr√©er les donn√©es.")


if __name__ == "__main__":
    main()
